{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[{"file_id":"https://github.com/AjiSiwi/arunika-temuin/blob/master/Machine%20Learning/RIASEC_Feature_Selection.ipynb","timestamp":1701181856479}]}},"cells":[{"cell_type":"code","metadata":{"id":"-hpqGP-SdD8e","executionInfo":{"status":"ok","timestamp":1701182067933,"user_tz":-420,"elapsed":6009,"user":{"displayName":"Nurul Andini M277BSX1256","userId":"07500154941621038947"}}},"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import KFold, train_test_split\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.metrics import accuracy_score, precision_score, recall_score"],"execution_count":1,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8btmlth5WijU","executionInfo":{"status":"ok","timestamp":1701182091008,"user_tz":-420,"elapsed":23090,"user":{"displayName":"Nurul Andini M277BSX1256","userId":"07500154941621038947"}},"outputId":"51873e61-2c1f-401b-d9fc-74b55df1408b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"ULhUzNhjdD8y","colab":{"base_uri":"https://localhost:8080/","height":499},"executionInfo":{"status":"ok","timestamp":1701182175471,"user_tz":-420,"elapsed":3422,"user":{"displayName":"Nurul Andini M277BSX1256","userId":"07500154941621038947"}},"outputId":"466ea91e-1b23-4f87-dd52-8410ff922ca9"},"source":["dataset = pd.read_excel('/content/drive/MyDrive/capstone_minatku/preprocessed_RIASEC_3Class.xlsx', engine = 'openpyxl')\n","dataset.head(n = 10)"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Unnamed: 0  R1  R2  R3  R4  R5  R6  R7  R8  I1  ...  TIPI2  TIPI3  TIPI4  \\\n","0           1   1   1   2   4   1   2   2   1   5  ...      5      7      4   \n","1           5   3   5   1   3   1   5   3   4   4  ...      2      3      5   \n","2           6   1   4   1   4   1   4   1   2   4  ...      2      5      5   \n","3          10   1   1   4   2   1   1   1   1   3  ...      5      7      1   \n","4          12   5   3   3   3   3   3   3   3   5  ...      3      7      5   \n","5          15   4   5   4   5   5   5   5   3   3  ...      2      5      6   \n","6          17   4   2   2   4   2   4   4   3   5  ...      6      7      6   \n","7          21   3   1   1   1   1   1   2   1   3  ...      5      7      7   \n","8          25   1   1   1   1   1   1   1   1   2  ...      6      5      5   \n","9          27   3   1   1   1   1   1   1   1   3  ...      5      3      7   \n","\n","   TIPI5  TIPI6  TIPI7  TIPI8  TIPI9  TIPI10                major  \n","0      7      6      6      4      6       1              science  \n","1      3      6      7      5      2       1  arts and literature  \n","2      3      6      2      4      7       2              science  \n","3      7      5      5      1      7       2            economics  \n","4      7      1      7      1      6       1               social  \n","5      7      4      5      3      5       3           technology  \n","6      5      5      6      2      1       1  arts and literature  \n","7      6      6      7      6      4       1            economics  \n","8      7      5      5      6      2       2  arts and literature  \n","9      7      7      5      5      1       1  arts and literature  \n","\n","[10 rows x 60 columns]"],"text/html":["\n","  <div id=\"df-f9cceef0-7ae4-4e2e-a349-80de84e8b69c\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>R1</th>\n","      <th>R2</th>\n","      <th>R3</th>\n","      <th>R4</th>\n","      <th>R5</th>\n","      <th>R6</th>\n","      <th>R7</th>\n","      <th>R8</th>\n","      <th>I1</th>\n","      <th>...</th>\n","      <th>TIPI2</th>\n","      <th>TIPI3</th>\n","      <th>TIPI4</th>\n","      <th>TIPI5</th>\n","      <th>TIPI6</th>\n","      <th>TIPI7</th>\n","      <th>TIPI8</th>\n","      <th>TIPI9</th>\n","      <th>TIPI10</th>\n","      <th>major</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>...</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>4</td>\n","      <td>7</td>\n","      <td>6</td>\n","      <td>6</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>science</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>6</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>arts and literature</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>6</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>7</td>\n","      <td>2</td>\n","      <td>science</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>7</td>\n","      <td>2</td>\n","      <td>economics</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>12</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>5</td>\n","      <td>...</td>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>social</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>15</td>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>6</td>\n","      <td>7</td>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>technology</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>17</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>5</td>\n","      <td>...</td>\n","      <td>6</td>\n","      <td>7</td>\n","      <td>6</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>6</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>arts and literature</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>21</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>7</td>\n","      <td>6</td>\n","      <td>6</td>\n","      <td>7</td>\n","      <td>6</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>economics</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>25</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>6</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>6</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>arts and literature</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>27</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>7</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>arts and literature</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10 rows × 60 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9cceef0-7ae4-4e2e-a349-80de84e8b69c')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-f9cceef0-7ae4-4e2e-a349-80de84e8b69c button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-f9cceef0-7ae4-4e2e-a349-80de84e8b69c');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-142eea40-4bef-4333-8cbb-7fdc0efe53b9\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-142eea40-4bef-4333-8cbb-7fdc0efe53b9')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-142eea40-4bef-4333-8cbb-7fdc0efe53b9 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"zm_nJmORdD83","executionInfo":{"status":"ok","timestamp":1701182175472,"user_tz":-420,"elapsed":42,"user":{"displayName":"Nurul Andini M277BSX1256","userId":"07500154941621038947"}}},"source":["# dataset.drop(columns = ['Unnamed: 0']) # Uncomment this line if filename == 'preprocessed_RIASEC_Class3.xlsx'\n","data_cols = list(dataset.columns)\n","data_cols.remove('major')\n","data = dataset[data_cols].values\n","labels = dataset['major'].values"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"9f1hHkYgdD85","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701182175473,"user_tz":-420,"elapsed":37,"user":{"displayName":"Nurul Andini M277BSX1256","userId":"07500154941621038947"}},"outputId":"c5f3fae6-f87d-4fc3-db45-c1372121b6cb"},"source":["labels = labels.reshape((1, labels.shape[0]))\n","transformer = ColumnTransformer([('one_hot_encoder', OneHotEncoder(sparse = False), [0])],\n","                                remainder = 'passthrough')\n","labels = transformer.fit_transform(labels.T)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]}]},{"cell_type":"code","metadata":{"id":"Zy6Kj1PidD88","executionInfo":{"status":"ok","timestamp":1701182642996,"user_tz":-420,"elapsed":370,"user":{"displayName":"Nurul Andini M277BSX1256","userId":"07500154941621038947"}}},"source":["def create_model(input_shape, learning_rate = 1e-5, dropout_rate = 0.5, optimizer = None):\n","    \"\"\"\n","    Create a three layers DNN model with 2 dropout layers for regularization.\n","\n","    Keyword Argument:\n","    input_shape -- a tuple defining the input_shape of the first layer. The value equals to the number of\n","                   features used.\n","    learning_rate -- defines the learning rate to be used for the default Adam optimizer. Default\n","                     value is 1e-4.\n","    dropout_rate -- defines the percentage of total weights to be dropped. Default value is 0.2,\n","                    meaning 20% of total weights are zeroed.\n","    optimizer -- if None, Adam will be used as default optimizer. Pass a tf.keras.optimizers method\n","                 to use another optimizer.\n","    \"\"\"\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Dense(units = 256, input_shape = input_shape, activation = 'relu'),\n","        tf.keras.layers.Dense(units = 128, activation = 'relu'),\n","        tf.keras.layers.Dense(units = 64, activation = 'relu'),\n","        tf.keras.layers.Dropout(dropout_rate),\n","        tf.keras.layers.Dense(units = 32, activation = 'relu'),\n","        tf.keras.layers.Dropout(dropout_rate),\n","        tf.keras.layers.Dense(units = 5, activation = 'softmax')\n","    ])\n","    if not optimizer:\n","        model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n","                      loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","    else:\n","        model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","    return model"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"bhv2eAPwdD9A","executionInfo":{"status":"ok","timestamp":1701182645646,"user_tz":-420,"elapsed":18,"user":{"displayName":"Nurul Andini M277BSX1256","userId":"07500154941621038947"}}},"source":["def predict_assess(model, validation_data, validation_label):\n","    \"\"\"\n","    Make predictions based on an array of data and assess the prediction using\n","    accuracy, precision, and recall.\n","\n","    Keyword Argument:\n","    model -- the model for making predictions.\n","\n","    validation_data -- an array of data for making predictions.\n","\n","    validation_label -- an array of labels of the data in validation_data.\n","    \"\"\"\n","    predictions = model.predict(validation_data)\n","    predictions = np.array([np.argmax(prediction) for prediction in predictions])\n","    actuals = np.array([np.argmax(actual) for actual in validation_label])\n","    accuracy = accuracy_score(predictions, actuals)\n","    precision = precision_score(predictions, actuals, average = 'weighted')\n","    recall = recall_score(predictions, actuals, average = 'weighted')\n","    return accuracy, precision, recall"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"iDw2s7ijdD9D","executionInfo":{"status":"ok","timestamp":1701182647962,"user_tz":-420,"elapsed":361,"user":{"displayName":"Nurul Andini M277BSX1256","userId":"07500154941621038947"}}},"source":["def training_report(accuracy, precision, recall, num_features = None):\n","    \"\"\"\n","    Print a training report based on model's performance.\n","\n","    Keyword Argument:\n","    accuracy -- the accuracy score of the model.\n","\n","    precision -- precision score.\n","\n","    recall -- recall score.\n","\n","    num_features -- a scalar value defining the number of features used during training.\n","                    If None, all features are assumed to be in use.\n","    \"\"\"\n","    if not num_features:\n","        print('Accuracy, Precision, and Recall for all features')\n","    else:\n","        print('Accuracy, Precision, and Recall for {} features'.format(num_features))\n","    print('Accuracy: {}'.format(acc))\n","    print('Precision: {}'.format(prec))\n","    print('Recall: {}'.format(rec))"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESTKRVvSdD9H","executionInfo":{"status":"ok","timestamp":1701182650502,"user_tz":-420,"elapsed":396,"user":{"displayName":"Nurul Andini M277BSX1256","userId":"07500154941621038947"}}},"source":["training_history = {'num_features': [], 'acc': [], 'prec': [], 'rec': []}\n","def add_to_history(hist_dict, n_features, acc, prec, rec):\n","    \"\"\"\n","    Add model's performance result to hist_dict dictionary containing 'num_features',\n","    'acc', 'prec', and 'rec' keys. The value of all keys are in lists.\n","\n","    Keyword Argument:\n","    hist_dict -- the dictionary where training performance will be saved. Must contain\n","                 'num_features', 'acc', 'prec', and 'rec' keys in which all of them must be\n","                 lists.\n","\n","    n_features -- number of features used during training. The value will be added to\n","                  'num_features'.\n","\n","    acc -- the obtained accuracy of a model.\n","\n","    prec -- precision score of a model.\n","\n","    rec -- recall score.\n","    \"\"\"\n","    hist_dict['num_features'].append(n_features)\n","    hist_dict['acc'].append(acc)\n","    hist_dict['prec'].append(prec)\n","    hist_dict['rec'].append(rec)"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"4hPZcdmFdD9O","executionInfo":{"status":"ok","timestamp":1701182653572,"user_tz":-420,"elapsed":9,"user":{"displayName":"Nurul Andini M277BSX1256","userId":"07500154941621038947"}}},"source":["data_train, data_test, label_train, label_test = train_test_split(data, labels, test_size = 0.25)"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"IVmhMmYwdD9T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701182822609,"user_tz":-420,"elapsed":79042,"user":{"displayName":"Nurul Andini M277BSX1256","userId":"07500154941621038947"}},"outputId":"e500de9f-1364-46ff-e4ec-80442ce64d7f"},"source":["model = create_model(input_shape = (len(dataset.columns) - 1, ),\n","                     optimizer = tf.keras.optimizers.SGD(learning_rate = 1e-3))\n","history = model.fit(data_train, label_train, batch_size = 32, epochs = 280, validation_data = (data_test, label_test))\n","acc, prec, rec = predict_assess(model, data_test, label_test)\n","training_report(acc, prec, rec)"],"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/280\n","61/61 [==============================] - 1s 10ms/step - loss: 106.5157 - accuracy: 0.2213 - val_loss: 1.6095 - val_accuracy: 0.2181\n","Epoch 2/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.6132 - accuracy: 0.2433 - val_loss: 1.6088 - val_accuracy: 0.2181\n","Epoch 3/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.6080 - accuracy: 0.2433 - val_loss: 1.6080 - val_accuracy: 0.2258\n","Epoch 4/280\n","61/61 [==============================] - 0s 7ms/step - loss: 1.6093 - accuracy: 0.2454 - val_loss: 1.6075 - val_accuracy: 0.2258\n","Epoch 5/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.6133 - accuracy: 0.2439 - val_loss: 1.6074 - val_accuracy: 0.2304\n","Epoch 6/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.6108 - accuracy: 0.2454 - val_loss: 1.6065 - val_accuracy: 0.2289\n","Epoch 7/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.6049 - accuracy: 0.2449 - val_loss: 1.6065 - val_accuracy: 0.2289\n","Epoch 8/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.6078 - accuracy: 0.2418 - val_loss: 1.6065 - val_accuracy: 0.2304\n","Epoch 9/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.6105 - accuracy: 0.2428 - val_loss: 1.6064 - val_accuracy: 0.2304\n","Epoch 10/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.6041 - accuracy: 0.2485 - val_loss: 1.6063 - val_accuracy: 0.2304\n","Epoch 11/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.6039 - accuracy: 0.2423 - val_loss: 1.6061 - val_accuracy: 0.2304\n","Epoch 12/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.6012 - accuracy: 0.2372 - val_loss: 1.6064 - val_accuracy: 0.2289\n","Epoch 13/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5998 - accuracy: 0.2464 - val_loss: 1.6062 - val_accuracy: 0.2304\n","Epoch 14/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.6035 - accuracy: 0.2490 - val_loss: 1.6061 - val_accuracy: 0.2304\n","Epoch 15/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.6005 - accuracy: 0.2439 - val_loss: 1.6060 - val_accuracy: 0.2304\n","Epoch 16/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.6013 - accuracy: 0.2490 - val_loss: 1.6061 - val_accuracy: 0.2304\n","Epoch 17/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.7012 - accuracy: 0.2531 - val_loss: 1.6060 - val_accuracy: 0.2289\n","Epoch 18/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.6007 - accuracy: 0.2510 - val_loss: 1.6060 - val_accuracy: 0.2289\n","Epoch 19/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.6014 - accuracy: 0.2392 - val_loss: 1.6060 - val_accuracy: 0.2289\n","Epoch 20/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.6035 - accuracy: 0.2459 - val_loss: 1.6058 - val_accuracy: 0.2289\n","Epoch 21/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.6008 - accuracy: 0.2459 - val_loss: 1.6056 - val_accuracy: 0.2289\n","Epoch 22/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5972 - accuracy: 0.2439 - val_loss: 1.6056 - val_accuracy: 0.2289\n","Epoch 23/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.6177 - accuracy: 0.2433 - val_loss: 1.6050 - val_accuracy: 0.2304\n","Epoch 24/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.6025 - accuracy: 0.2382 - val_loss: 1.6050 - val_accuracy: 0.2304\n","Epoch 25/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5961 - accuracy: 0.2454 - val_loss: 1.6052 - val_accuracy: 0.2304\n","Epoch 26/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.6000 - accuracy: 0.2413 - val_loss: 1.6050 - val_accuracy: 0.2304\n","Epoch 27/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5990 - accuracy: 0.2449 - val_loss: 1.6049 - val_accuracy: 0.2304\n","Epoch 28/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.6004 - accuracy: 0.2398 - val_loss: 1.6048 - val_accuracy: 0.2304\n","Epoch 29/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5995 - accuracy: 0.2377 - val_loss: 1.6045 - val_accuracy: 0.2304\n","Epoch 30/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.6002 - accuracy: 0.2357 - val_loss: 1.6043 - val_accuracy: 0.2304\n","Epoch 31/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5974 - accuracy: 0.2403 - val_loss: 1.6043 - val_accuracy: 0.2304\n","Epoch 32/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.6052 - accuracy: 0.2418 - val_loss: 1.6042 - val_accuracy: 0.2289\n","Epoch 33/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5950 - accuracy: 0.2464 - val_loss: 1.6043 - val_accuracy: 0.2289\n","Epoch 34/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.6023 - accuracy: 0.2526 - val_loss: 1.6043 - val_accuracy: 0.2289\n","Epoch 35/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5944 - accuracy: 0.2428 - val_loss: 1.6044 - val_accuracy: 0.2289\n","Epoch 36/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5954 - accuracy: 0.2413 - val_loss: 1.6044 - val_accuracy: 0.2289\n","Epoch 37/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.6008 - accuracy: 0.2387 - val_loss: 1.6043 - val_accuracy: 0.2304\n","Epoch 38/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5984 - accuracy: 0.2439 - val_loss: 1.6043 - val_accuracy: 0.2289\n","Epoch 39/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5941 - accuracy: 0.2459 - val_loss: 1.6042 - val_accuracy: 0.2304\n","Epoch 40/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5977 - accuracy: 0.2362 - val_loss: 1.6044 - val_accuracy: 0.2320\n","Epoch 41/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5979 - accuracy: 0.2531 - val_loss: 1.6044 - val_accuracy: 0.2289\n","Epoch 42/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5978 - accuracy: 0.2485 - val_loss: 1.6043 - val_accuracy: 0.2289\n","Epoch 43/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5950 - accuracy: 0.2459 - val_loss: 1.6042 - val_accuracy: 0.2289\n","Epoch 44/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.6030 - accuracy: 0.2454 - val_loss: 1.6039 - val_accuracy: 0.2304\n","Epoch 45/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5975 - accuracy: 0.2367 - val_loss: 1.6040 - val_accuracy: 0.2304\n","Epoch 46/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5980 - accuracy: 0.2413 - val_loss: 1.6041 - val_accuracy: 0.2304\n","Epoch 47/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5960 - accuracy: 0.2500 - val_loss: 1.6042 - val_accuracy: 0.2289\n","Epoch 48/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5962 - accuracy: 0.2433 - val_loss: 1.6041 - val_accuracy: 0.2289\n","Epoch 49/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5955 - accuracy: 0.2526 - val_loss: 1.6042 - val_accuracy: 0.2289\n","Epoch 50/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5967 - accuracy: 0.2408 - val_loss: 1.6041 - val_accuracy: 0.2289\n","Epoch 51/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.6000 - accuracy: 0.2418 - val_loss: 1.6038 - val_accuracy: 0.2289\n","Epoch 52/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5955 - accuracy: 0.2592 - val_loss: 1.6041 - val_accuracy: 0.2289\n","Epoch 53/280\n","61/61 [==============================] - 0s 7ms/step - loss: 1.5979 - accuracy: 0.2382 - val_loss: 1.6040 - val_accuracy: 0.2289\n","Epoch 54/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5948 - accuracy: 0.2387 - val_loss: 1.6040 - val_accuracy: 0.2289\n","Epoch 55/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5957 - accuracy: 0.2351 - val_loss: 1.6042 - val_accuracy: 0.2289\n","Epoch 56/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5983 - accuracy: 0.2480 - val_loss: 1.6038 - val_accuracy: 0.2304\n","Epoch 57/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5974 - accuracy: 0.2454 - val_loss: 1.6037 - val_accuracy: 0.2304\n","Epoch 58/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5950 - accuracy: 0.2433 - val_loss: 1.6039 - val_accuracy: 0.2304\n","Epoch 59/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5963 - accuracy: 0.2408 - val_loss: 1.6039 - val_accuracy: 0.2304\n","Epoch 60/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5974 - accuracy: 0.2433 - val_loss: 1.6039 - val_accuracy: 0.2304\n","Epoch 61/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5965 - accuracy: 0.2469 - val_loss: 1.6042 - val_accuracy: 0.2289\n","Epoch 62/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5963 - accuracy: 0.2423 - val_loss: 1.6041 - val_accuracy: 0.2304\n","Epoch 63/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5956 - accuracy: 0.2469 - val_loss: 1.6042 - val_accuracy: 0.2304\n","Epoch 64/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5948 - accuracy: 0.2474 - val_loss: 1.6042 - val_accuracy: 0.2304\n","Epoch 65/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5955 - accuracy: 0.2485 - val_loss: 1.6040 - val_accuracy: 0.2304\n","Epoch 66/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5948 - accuracy: 0.2449 - val_loss: 1.6036 - val_accuracy: 0.2304\n","Epoch 67/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5956 - accuracy: 0.2346 - val_loss: 1.6038 - val_accuracy: 0.2304\n","Epoch 68/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5964 - accuracy: 0.2505 - val_loss: 1.6039 - val_accuracy: 0.2320\n","Epoch 69/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5940 - accuracy: 0.2572 - val_loss: 1.6042 - val_accuracy: 0.2320\n","Epoch 70/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5961 - accuracy: 0.2464 - val_loss: 1.6046 - val_accuracy: 0.2304\n","Epoch 71/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5986 - accuracy: 0.2269 - val_loss: 1.6043 - val_accuracy: 0.2304\n","Epoch 72/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5969 - accuracy: 0.2418 - val_loss: 1.6041 - val_accuracy: 0.2289\n","Epoch 73/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5962 - accuracy: 0.2398 - val_loss: 1.6040 - val_accuracy: 0.2289\n","Epoch 74/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5965 - accuracy: 0.2182 - val_loss: 1.6039 - val_accuracy: 0.2289\n","Epoch 75/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5960 - accuracy: 0.2541 - val_loss: 1.6042 - val_accuracy: 0.2289\n","Epoch 76/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5933 - accuracy: 0.2480 - val_loss: 1.6043 - val_accuracy: 0.2289\n","Epoch 77/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5944 - accuracy: 0.2433 - val_loss: 1.6044 - val_accuracy: 0.2289\n","Epoch 78/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5946 - accuracy: 0.2428 - val_loss: 1.6044 - val_accuracy: 0.2304\n","Epoch 79/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5939 - accuracy: 0.2428 - val_loss: 1.6045 - val_accuracy: 0.2304\n","Epoch 80/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5951 - accuracy: 0.2464 - val_loss: 1.6045 - val_accuracy: 0.2304\n","Epoch 81/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5929 - accuracy: 0.2377 - val_loss: 1.6045 - val_accuracy: 0.2304\n","Epoch 82/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5928 - accuracy: 0.2464 - val_loss: 1.6045 - val_accuracy: 0.2304\n","Epoch 83/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5947 - accuracy: 0.2346 - val_loss: 1.6044 - val_accuracy: 0.2304\n","Epoch 84/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5955 - accuracy: 0.2449 - val_loss: 1.6043 - val_accuracy: 0.2289\n","Epoch 85/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5974 - accuracy: 0.2418 - val_loss: 1.6041 - val_accuracy: 0.2289\n","Epoch 86/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5943 - accuracy: 0.2321 - val_loss: 1.6041 - val_accuracy: 0.2304\n","Epoch 87/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5924 - accuracy: 0.2454 - val_loss: 1.6043 - val_accuracy: 0.2304\n","Epoch 88/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5952 - accuracy: 0.2413 - val_loss: 1.6042 - val_accuracy: 0.2304\n","Epoch 89/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5951 - accuracy: 0.2387 - val_loss: 1.6045 - val_accuracy: 0.2304\n","Epoch 90/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5941 - accuracy: 0.2413 - val_loss: 1.6045 - val_accuracy: 0.2304\n","Epoch 91/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5949 - accuracy: 0.2433 - val_loss: 1.6045 - val_accuracy: 0.2304\n","Epoch 92/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5964 - accuracy: 0.2418 - val_loss: 1.6043 - val_accuracy: 0.2304\n","Epoch 93/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5935 - accuracy: 0.2490 - val_loss: 1.6045 - val_accuracy: 0.2304\n","Epoch 94/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5948 - accuracy: 0.2316 - val_loss: 1.6045 - val_accuracy: 0.2273\n","Epoch 95/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5960 - accuracy: 0.2428 - val_loss: 1.6044 - val_accuracy: 0.2273\n","Epoch 96/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5945 - accuracy: 0.2362 - val_loss: 1.6041 - val_accuracy: 0.2289\n","Epoch 97/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5929 - accuracy: 0.2531 - val_loss: 1.6041 - val_accuracy: 0.2289\n","Epoch 98/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5973 - accuracy: 0.2398 - val_loss: 1.6040 - val_accuracy: 0.2289\n","Epoch 99/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5957 - accuracy: 0.2316 - val_loss: 1.6041 - val_accuracy: 0.2289\n","Epoch 100/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5959 - accuracy: 0.2490 - val_loss: 1.6045 - val_accuracy: 0.2273\n","Epoch 101/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5935 - accuracy: 0.2495 - val_loss: 1.6045 - val_accuracy: 0.2273\n","Epoch 102/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5927 - accuracy: 0.2408 - val_loss: 1.6045 - val_accuracy: 0.2273\n","Epoch 103/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5932 - accuracy: 0.2510 - val_loss: 1.6045 - val_accuracy: 0.2273\n","Epoch 104/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5935 - accuracy: 0.2505 - val_loss: 1.6046 - val_accuracy: 0.2289\n","Epoch 105/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5901 - accuracy: 0.2602 - val_loss: 1.6048 - val_accuracy: 0.2289\n","Epoch 106/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5944 - accuracy: 0.2474 - val_loss: 1.6049 - val_accuracy: 0.2273\n","Epoch 107/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5921 - accuracy: 0.2577 - val_loss: 1.6046 - val_accuracy: 0.2304\n","Epoch 108/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5923 - accuracy: 0.2439 - val_loss: 1.6049 - val_accuracy: 0.2289\n","Epoch 109/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5975 - accuracy: 0.2362 - val_loss: 1.6044 - val_accuracy: 0.2273\n","Epoch 110/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5938 - accuracy: 0.2480 - val_loss: 1.6043 - val_accuracy: 0.2273\n","Epoch 111/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5966 - accuracy: 0.2341 - val_loss: 1.6043 - val_accuracy: 0.2273\n","Epoch 112/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5936 - accuracy: 0.2474 - val_loss: 1.6042 - val_accuracy: 0.2273\n","Epoch 113/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5947 - accuracy: 0.2428 - val_loss: 1.6041 - val_accuracy: 0.2273\n","Epoch 114/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5950 - accuracy: 0.2474 - val_loss: 1.6041 - val_accuracy: 0.2273\n","Epoch 115/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5955 - accuracy: 0.2423 - val_loss: 1.6042 - val_accuracy: 0.2273\n","Epoch 116/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5925 - accuracy: 0.2551 - val_loss: 1.6044 - val_accuracy: 0.2273\n","Epoch 117/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5943 - accuracy: 0.2392 - val_loss: 1.6044 - val_accuracy: 0.2273\n","Epoch 118/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5931 - accuracy: 0.2403 - val_loss: 1.6045 - val_accuracy: 0.2273\n","Epoch 119/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5957 - accuracy: 0.2408 - val_loss: 1.6047 - val_accuracy: 0.2273\n","Epoch 120/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5921 - accuracy: 0.2531 - val_loss: 1.6047 - val_accuracy: 0.2273\n","Epoch 121/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5920 - accuracy: 0.2367 - val_loss: 1.6048 - val_accuracy: 0.2273\n","Epoch 122/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5938 - accuracy: 0.2403 - val_loss: 1.6048 - val_accuracy: 0.2273\n","Epoch 123/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5924 - accuracy: 0.2526 - val_loss: 1.6048 - val_accuracy: 0.2273\n","Epoch 124/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5945 - accuracy: 0.2357 - val_loss: 1.6047 - val_accuracy: 0.2273\n","Epoch 125/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5932 - accuracy: 0.2536 - val_loss: 1.6049 - val_accuracy: 0.2273\n","Epoch 126/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5928 - accuracy: 0.2480 - val_loss: 1.6049 - val_accuracy: 0.2273\n","Epoch 127/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5939 - accuracy: 0.2372 - val_loss: 1.6049 - val_accuracy: 0.2273\n","Epoch 128/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5952 - accuracy: 0.2444 - val_loss: 1.6048 - val_accuracy: 0.2273\n","Epoch 129/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5949 - accuracy: 0.2285 - val_loss: 1.6049 - val_accuracy: 0.2273\n","Epoch 130/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5927 - accuracy: 0.2490 - val_loss: 1.6052 - val_accuracy: 0.2273\n","Epoch 131/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5937 - accuracy: 0.2515 - val_loss: 1.6049 - val_accuracy: 0.2289\n","Epoch 132/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5944 - accuracy: 0.2398 - val_loss: 1.6043 - val_accuracy: 0.2289\n","Epoch 133/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5910 - accuracy: 0.2464 - val_loss: 1.6047 - val_accuracy: 0.2289\n","Epoch 134/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5913 - accuracy: 0.2433 - val_loss: 1.6051 - val_accuracy: 0.2289\n","Epoch 135/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5933 - accuracy: 0.2485 - val_loss: 1.6056 - val_accuracy: 0.2289\n","Epoch 136/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5932 - accuracy: 0.2531 - val_loss: 1.6055 - val_accuracy: 0.2289\n","Epoch 137/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5941 - accuracy: 0.2433 - val_loss: 1.6053 - val_accuracy: 0.2273\n","Epoch 138/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5924 - accuracy: 0.2480 - val_loss: 1.6053 - val_accuracy: 0.2273\n","Epoch 139/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5938 - accuracy: 0.2439 - val_loss: 1.6051 - val_accuracy: 0.2289\n","Epoch 140/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5926 - accuracy: 0.2403 - val_loss: 1.6046 - val_accuracy: 0.2289\n","Epoch 141/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5929 - accuracy: 0.2469 - val_loss: 1.6048 - val_accuracy: 0.2289\n","Epoch 142/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5943 - accuracy: 0.2449 - val_loss: 1.6050 - val_accuracy: 0.2289\n","Epoch 143/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5946 - accuracy: 0.2459 - val_loss: 1.6049 - val_accuracy: 0.2289\n","Epoch 144/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5936 - accuracy: 0.2449 - val_loss: 1.6050 - val_accuracy: 0.2289\n","Epoch 145/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5908 - accuracy: 0.2413 - val_loss: 1.6053 - val_accuracy: 0.2289\n","Epoch 146/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5930 - accuracy: 0.2485 - val_loss: 1.6048 - val_accuracy: 0.2304\n","Epoch 147/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5926 - accuracy: 0.2423 - val_loss: 1.6051 - val_accuracy: 0.2304\n","Epoch 148/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5935 - accuracy: 0.2341 - val_loss: 1.6051 - val_accuracy: 0.2289\n","Epoch 149/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5923 - accuracy: 0.2454 - val_loss: 1.6051 - val_accuracy: 0.2273\n","Epoch 150/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5965 - accuracy: 0.2351 - val_loss: 1.6047 - val_accuracy: 0.2289\n","Epoch 151/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5921 - accuracy: 0.2561 - val_loss: 1.6050 - val_accuracy: 0.2273\n","Epoch 152/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5929 - accuracy: 0.2382 - val_loss: 1.6051 - val_accuracy: 0.2273\n","Epoch 153/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5925 - accuracy: 0.2541 - val_loss: 1.6053 - val_accuracy: 0.2273\n","Epoch 154/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5905 - accuracy: 0.2582 - val_loss: 1.6056 - val_accuracy: 0.2289\n","Epoch 155/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5920 - accuracy: 0.2526 - val_loss: 1.6054 - val_accuracy: 0.2289\n","Epoch 156/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5950 - accuracy: 0.2480 - val_loss: 1.6051 - val_accuracy: 0.2273\n","Epoch 157/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5939 - accuracy: 0.2418 - val_loss: 1.6046 - val_accuracy: 0.2304\n","Epoch 158/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5944 - accuracy: 0.2546 - val_loss: 1.6046 - val_accuracy: 0.2289\n","Epoch 159/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5924 - accuracy: 0.2510 - val_loss: 1.6048 - val_accuracy: 0.2289\n","Epoch 160/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5916 - accuracy: 0.2418 - val_loss: 1.6046 - val_accuracy: 0.2273\n","Epoch 161/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5937 - accuracy: 0.2469 - val_loss: 1.6047 - val_accuracy: 0.2273\n","Epoch 162/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5924 - accuracy: 0.2418 - val_loss: 1.6048 - val_accuracy: 0.2289\n","Epoch 163/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5949 - accuracy: 0.2520 - val_loss: 1.6048 - val_accuracy: 0.2289\n","Epoch 164/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5950 - accuracy: 0.2341 - val_loss: 1.6047 - val_accuracy: 0.2289\n","Epoch 165/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5954 - accuracy: 0.2372 - val_loss: 1.6046 - val_accuracy: 0.2289\n","Epoch 166/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5934 - accuracy: 0.2310 - val_loss: 1.6048 - val_accuracy: 0.2289\n","Epoch 167/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5939 - accuracy: 0.2387 - val_loss: 1.6049 - val_accuracy: 0.2289\n","Epoch 168/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5914 - accuracy: 0.2346 - val_loss: 1.6051 - val_accuracy: 0.2289\n","Epoch 169/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5937 - accuracy: 0.2433 - val_loss: 1.6046 - val_accuracy: 0.2320\n","Epoch 170/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5933 - accuracy: 0.2444 - val_loss: 1.6048 - val_accuracy: 0.2320\n","Epoch 171/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5945 - accuracy: 0.2295 - val_loss: 1.6046 - val_accuracy: 0.2304\n","Epoch 172/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5928 - accuracy: 0.2515 - val_loss: 1.6048 - val_accuracy: 0.2320\n","Epoch 173/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5965 - accuracy: 0.2490 - val_loss: 1.6049 - val_accuracy: 0.2320\n","Epoch 174/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5928 - accuracy: 0.2346 - val_loss: 1.6045 - val_accuracy: 0.2320\n","Epoch 175/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5912 - accuracy: 0.2526 - val_loss: 1.6046 - val_accuracy: 0.2320\n","Epoch 176/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5960 - accuracy: 0.2295 - val_loss: 1.6045 - val_accuracy: 0.2320\n","Epoch 177/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5922 - accuracy: 0.2459 - val_loss: 1.6045 - val_accuracy: 0.2320\n","Epoch 178/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5940 - accuracy: 0.2316 - val_loss: 1.6044 - val_accuracy: 0.2304\n","Epoch 179/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5945 - accuracy: 0.2351 - val_loss: 1.6045 - val_accuracy: 0.2304\n","Epoch 180/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5939 - accuracy: 0.2392 - val_loss: 1.6045 - val_accuracy: 0.2304\n","Epoch 181/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5922 - accuracy: 0.2377 - val_loss: 1.6044 - val_accuracy: 0.2304\n","Epoch 182/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5940 - accuracy: 0.2362 - val_loss: 1.6043 - val_accuracy: 0.2304\n","Epoch 183/280\n","61/61 [==============================] - 0s 8ms/step - loss: 1.5932 - accuracy: 0.2367 - val_loss: 1.6044 - val_accuracy: 0.2304\n","Epoch 184/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5942 - accuracy: 0.2403 - val_loss: 1.6045 - val_accuracy: 0.2304\n","Epoch 185/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5926 - accuracy: 0.2480 - val_loss: 1.6043 - val_accuracy: 0.2304\n","Epoch 186/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5926 - accuracy: 0.2346 - val_loss: 1.6044 - val_accuracy: 0.2304\n","Epoch 187/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5950 - accuracy: 0.2336 - val_loss: 1.6045 - val_accuracy: 0.2304\n","Epoch 188/280\n","61/61 [==============================] - 0s 7ms/step - loss: 1.5933 - accuracy: 0.2403 - val_loss: 1.6045 - val_accuracy: 0.2304\n","Epoch 189/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5932 - accuracy: 0.2423 - val_loss: 1.6047 - val_accuracy: 0.2304\n","Epoch 190/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5926 - accuracy: 0.2459 - val_loss: 1.6047 - val_accuracy: 0.2304\n","Epoch 191/280\n","61/61 [==============================] - 0s 7ms/step - loss: 1.5935 - accuracy: 0.2500 - val_loss: 1.6046 - val_accuracy: 0.2304\n","Epoch 192/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5931 - accuracy: 0.2351 - val_loss: 1.6047 - val_accuracy: 0.2320\n","Epoch 193/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5932 - accuracy: 0.2362 - val_loss: 1.6049 - val_accuracy: 0.2320\n","Epoch 194/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5928 - accuracy: 0.2367 - val_loss: 1.6048 - val_accuracy: 0.2304\n","Epoch 195/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5928 - accuracy: 0.2495 - val_loss: 1.6048 - val_accuracy: 0.2304\n","Epoch 196/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5937 - accuracy: 0.2480 - val_loss: 1.6051 - val_accuracy: 0.2289\n","Epoch 197/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5923 - accuracy: 0.2408 - val_loss: 1.6054 - val_accuracy: 0.2273\n","Epoch 198/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5930 - accuracy: 0.2418 - val_loss: 1.6053 - val_accuracy: 0.2289\n","Epoch 199/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5929 - accuracy: 0.2444 - val_loss: 1.6056 - val_accuracy: 0.2273\n","Epoch 200/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5943 - accuracy: 0.2392 - val_loss: 1.6051 - val_accuracy: 0.2304\n","Epoch 201/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5937 - accuracy: 0.2531 - val_loss: 1.6049 - val_accuracy: 0.2304\n","Epoch 202/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5941 - accuracy: 0.2382 - val_loss: 1.6048 - val_accuracy: 0.2289\n","Epoch 203/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5919 - accuracy: 0.2362 - val_loss: 1.6057 - val_accuracy: 0.2243\n","Epoch 204/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.6002 - accuracy: 0.2362 - val_loss: 1.6051 - val_accuracy: 0.2304\n","Epoch 205/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5929 - accuracy: 0.2469 - val_loss: 1.6049 - val_accuracy: 0.2289\n","Epoch 206/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5908 - accuracy: 0.2464 - val_loss: 1.6050 - val_accuracy: 0.2289\n","Epoch 207/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5933 - accuracy: 0.2433 - val_loss: 1.6052 - val_accuracy: 0.2289\n","Epoch 208/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5924 - accuracy: 0.2418 - val_loss: 1.6052 - val_accuracy: 0.2289\n","Epoch 209/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5934 - accuracy: 0.2413 - val_loss: 1.6051 - val_accuracy: 0.2289\n","Epoch 210/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5952 - accuracy: 0.2444 - val_loss: 1.6049 - val_accuracy: 0.2289\n","Epoch 211/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5920 - accuracy: 0.2398 - val_loss: 1.6051 - val_accuracy: 0.2289\n","Epoch 212/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5935 - accuracy: 0.2403 - val_loss: 1.6051 - val_accuracy: 0.2289\n","Epoch 213/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5926 - accuracy: 0.2392 - val_loss: 1.6050 - val_accuracy: 0.2289\n","Epoch 214/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5934 - accuracy: 0.2372 - val_loss: 1.6050 - val_accuracy: 0.2304\n","Epoch 215/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5924 - accuracy: 0.2357 - val_loss: 1.6049 - val_accuracy: 0.2304\n","Epoch 216/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5935 - accuracy: 0.2408 - val_loss: 1.6048 - val_accuracy: 0.2304\n","Epoch 217/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5942 - accuracy: 0.2341 - val_loss: 1.6049 - val_accuracy: 0.2304\n","Epoch 218/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5945 - accuracy: 0.2505 - val_loss: 1.6046 - val_accuracy: 0.2304\n","Epoch 219/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5928 - accuracy: 0.2531 - val_loss: 1.6050 - val_accuracy: 0.2304\n","Epoch 220/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5935 - accuracy: 0.2418 - val_loss: 1.6052 - val_accuracy: 0.2304\n","Epoch 221/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5918 - accuracy: 0.2454 - val_loss: 1.6052 - val_accuracy: 0.2304\n","Epoch 222/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5935 - accuracy: 0.2387 - val_loss: 1.6051 - val_accuracy: 0.2304\n","Epoch 223/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5924 - accuracy: 0.2495 - val_loss: 1.6053 - val_accuracy: 0.2304\n","Epoch 224/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5920 - accuracy: 0.2428 - val_loss: 1.6051 - val_accuracy: 0.2304\n","Epoch 225/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5955 - accuracy: 0.2367 - val_loss: 1.6048 - val_accuracy: 0.2304\n","Epoch 226/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5946 - accuracy: 0.2300 - val_loss: 1.6046 - val_accuracy: 0.2304\n","Epoch 227/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5934 - accuracy: 0.2346 - val_loss: 1.6046 - val_accuracy: 0.2304\n","Epoch 228/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5922 - accuracy: 0.2520 - val_loss: 1.6047 - val_accuracy: 0.2304\n","Epoch 229/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5934 - accuracy: 0.2520 - val_loss: 1.6048 - val_accuracy: 0.2304\n","Epoch 230/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5942 - accuracy: 0.2449 - val_loss: 1.6043 - val_accuracy: 0.2320\n","Epoch 231/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5930 - accuracy: 0.2495 - val_loss: 1.6044 - val_accuracy: 0.2320\n","Epoch 232/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5930 - accuracy: 0.2336 - val_loss: 1.6045 - val_accuracy: 0.2304\n","Epoch 233/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5943 - accuracy: 0.2464 - val_loss: 1.6045 - val_accuracy: 0.2304\n","Epoch 234/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5945 - accuracy: 0.2372 - val_loss: 1.6045 - val_accuracy: 0.2304\n","Epoch 235/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5934 - accuracy: 0.2351 - val_loss: 1.6045 - val_accuracy: 0.2304\n","Epoch 236/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5935 - accuracy: 0.2515 - val_loss: 1.6045 - val_accuracy: 0.2304\n","Epoch 237/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5924 - accuracy: 0.2546 - val_loss: 1.6045 - val_accuracy: 0.2304\n","Epoch 238/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5929 - accuracy: 0.2459 - val_loss: 1.6044 - val_accuracy: 0.2304\n","Epoch 239/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5934 - accuracy: 0.2433 - val_loss: 1.6044 - val_accuracy: 0.2304\n","Epoch 240/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5945 - accuracy: 0.2275 - val_loss: 1.6043 - val_accuracy: 0.2304\n","Epoch 241/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5922 - accuracy: 0.2362 - val_loss: 1.6045 - val_accuracy: 0.2304\n","Epoch 242/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5961 - accuracy: 0.2439 - val_loss: 1.6046 - val_accuracy: 0.2304\n","Epoch 243/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5954 - accuracy: 0.2444 - val_loss: 1.6047 - val_accuracy: 0.2304\n","Epoch 244/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5923 - accuracy: 0.2505 - val_loss: 1.6046 - val_accuracy: 0.2320\n","Epoch 245/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5936 - accuracy: 0.2413 - val_loss: 1.6048 - val_accuracy: 0.2320\n","Epoch 246/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5913 - accuracy: 0.2500 - val_loss: 1.6048 - val_accuracy: 0.2320\n","Epoch 247/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5910 - accuracy: 0.2418 - val_loss: 1.6048 - val_accuracy: 0.2320\n","Epoch 248/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5945 - accuracy: 0.2546 - val_loss: 1.6048 - val_accuracy: 0.2320\n","Epoch 249/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5923 - accuracy: 0.2490 - val_loss: 1.6052 - val_accuracy: 0.2289\n","Epoch 250/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5925 - accuracy: 0.2428 - val_loss: 1.6067 - val_accuracy: 0.2120\n","Epoch 251/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5949 - accuracy: 0.2485 - val_loss: 1.6047 - val_accuracy: 0.2304\n","Epoch 252/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5939 - accuracy: 0.2310 - val_loss: 1.6046 - val_accuracy: 0.2304\n","Epoch 253/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5935 - accuracy: 0.2336 - val_loss: 1.6047 - val_accuracy: 0.2304\n","Epoch 254/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5923 - accuracy: 0.2567 - val_loss: 1.6048 - val_accuracy: 0.2304\n","Epoch 255/280\n","61/61 [==============================] - 0s 6ms/step - loss: 1.5917 - accuracy: 0.2418 - val_loss: 1.6049 - val_accuracy: 0.2304\n","Epoch 256/280\n","61/61 [==============================] - 0s 5ms/step - loss: 1.5934 - accuracy: 0.2444 - val_loss: 1.6050 - val_accuracy: 0.2289\n","Epoch 257/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5935 - accuracy: 0.2428 - val_loss: 1.6050 - val_accuracy: 0.2289\n","Epoch 258/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5941 - accuracy: 0.2490 - val_loss: 1.6051 - val_accuracy: 0.2304\n","Epoch 259/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5929 - accuracy: 0.2418 - val_loss: 1.6050 - val_accuracy: 0.2289\n","Epoch 260/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5920 - accuracy: 0.2500 - val_loss: 1.6050 - val_accuracy: 0.2304\n","Epoch 261/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5919 - accuracy: 0.2526 - val_loss: 1.6051 - val_accuracy: 0.2304\n","Epoch 262/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5933 - accuracy: 0.2321 - val_loss: 1.6051 - val_accuracy: 0.2304\n","Epoch 263/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5929 - accuracy: 0.2408 - val_loss: 1.6053 - val_accuracy: 0.2304\n","Epoch 264/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5931 - accuracy: 0.2485 - val_loss: 1.6053 - val_accuracy: 0.2304\n","Epoch 265/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5936 - accuracy: 0.2480 - val_loss: 1.6052 - val_accuracy: 0.2304\n","Epoch 266/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5931 - accuracy: 0.2346 - val_loss: 1.6051 - val_accuracy: 0.2304\n","Epoch 267/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5932 - accuracy: 0.2433 - val_loss: 1.6050 - val_accuracy: 0.2304\n","Epoch 268/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5916 - accuracy: 0.2572 - val_loss: 1.6051 - val_accuracy: 0.2304\n","Epoch 269/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5931 - accuracy: 0.2515 - val_loss: 1.6052 - val_accuracy: 0.2304\n","Epoch 270/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5918 - accuracy: 0.2439 - val_loss: 1.6052 - val_accuracy: 0.2289\n","Epoch 271/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5926 - accuracy: 0.2382 - val_loss: 1.6051 - val_accuracy: 0.2289\n","Epoch 272/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5909 - accuracy: 0.2505 - val_loss: 1.6050 - val_accuracy: 0.2289\n","Epoch 273/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5940 - accuracy: 0.2480 - val_loss: 1.6053 - val_accuracy: 0.2289\n","Epoch 274/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5930 - accuracy: 0.2362 - val_loss: 1.6052 - val_accuracy: 0.2289\n","Epoch 275/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5928 - accuracy: 0.2408 - val_loss: 1.6050 - val_accuracy: 0.2289\n","Epoch 276/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5919 - accuracy: 0.2372 - val_loss: 1.6052 - val_accuracy: 0.2289\n","Epoch 277/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5926 - accuracy: 0.2577 - val_loss: 1.6053 - val_accuracy: 0.2289\n","Epoch 278/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5946 - accuracy: 0.2403 - val_loss: 1.6050 - val_accuracy: 0.2289\n","Epoch 279/280\n","61/61 [==============================] - 0s 3ms/step - loss: 1.5936 - accuracy: 0.2254 - val_loss: 1.6052 - val_accuracy: 0.2289\n","Epoch 280/280\n","61/61 [==============================] - 0s 4ms/step - loss: 1.5931 - accuracy: 0.2346 - val_loss: 1.6051 - val_accuracy: 0.2289\n","21/21 [==============================] - 0s 2ms/step\n","Accuracy, Precision, and Recall for all features\n","Accuracy: 0.22887864823348694\n","Precision: 0.9606647892131764\n","Recall: 0.22887864823348694\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","metadata":{"id":"RvWgr7x_dD9V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701182917061,"user_tz":-420,"elapsed":94503,"user":{"displayName":"Nurul Andini M277BSX1256","userId":"07500154941621038947"}},"outputId":"1d89a9fa-1f03-4612-8966-6f651df1779e"},"source":["k_fold = KFold(n_splits = 5)\n","for train_index, test_index in k_fold.split(data):\n","    data_train, data_test = data[train_index], data[test_index]\n","    label_train, label_test = labels[train_index], labels[test_index]\n","    model = create_model(input_shape = (len(dataset.columns) - 1, ))\n","    history = model.fit(data_train, label_train, batch_size = 32, epochs = 50)\n","    acc, prec, rec = predict_assess(model, data_test, label_test)\n","    training_report(acc, prec, rec)\n","    add_to_history(training_history, len(dataset.columns), acc, prec, rec)"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","66/66 [==============================] - 2s 3ms/step - loss: 443.2243 - accuracy: 0.1979\n","Epoch 2/50\n","66/66 [==============================] - 0s 3ms/step - loss: 360.7437 - accuracy: 0.1787\n","Epoch 3/50\n","66/66 [==============================] - 0s 3ms/step - loss: 309.7514 - accuracy: 0.1878\n","Epoch 4/50\n","66/66 [==============================] - 0s 3ms/step - loss: 254.7833 - accuracy: 0.1825\n","Epoch 5/50\n","66/66 [==============================] - 0s 3ms/step - loss: 222.7438 - accuracy: 0.1940\n","Epoch 6/50\n","66/66 [==============================] - 0s 3ms/step - loss: 191.8794 - accuracy: 0.1940\n","Epoch 7/50\n","66/66 [==============================] - 0s 3ms/step - loss: 165.4092 - accuracy: 0.1854\n","Epoch 8/50\n","66/66 [==============================] - 0s 3ms/step - loss: 139.5085 - accuracy: 0.1931\n","Epoch 9/50\n","66/66 [==============================] - 0s 3ms/step - loss: 130.3552 - accuracy: 0.1844\n","Epoch 10/50\n","66/66 [==============================] - 0s 3ms/step - loss: 107.9572 - accuracy: 0.2037\n","Epoch 11/50\n","66/66 [==============================] - 0s 3ms/step - loss: 95.5826 - accuracy: 0.1984\n","Epoch 12/50\n","66/66 [==============================] - 0s 4ms/step - loss: 81.3932 - accuracy: 0.1960\n","Epoch 13/50\n","66/66 [==============================] - 0s 4ms/step - loss: 71.9318 - accuracy: 0.1878\n","Epoch 14/50\n","66/66 [==============================] - 0s 5ms/step - loss: 59.5230 - accuracy: 0.2022\n","Epoch 15/50\n","66/66 [==============================] - 0s 5ms/step - loss: 49.0350 - accuracy: 0.1984\n","Epoch 16/50\n","66/66 [==============================] - 0s 5ms/step - loss: 42.8895 - accuracy: 0.2046\n","Epoch 17/50\n","66/66 [==============================] - 0s 5ms/step - loss: 35.6612 - accuracy: 0.1964\n","Epoch 18/50\n","66/66 [==============================] - 0s 5ms/step - loss: 28.3980 - accuracy: 0.2051\n","Epoch 19/50\n","66/66 [==============================] - 0s 5ms/step - loss: 20.7120 - accuracy: 0.2003\n","Epoch 20/50\n","66/66 [==============================] - 0s 5ms/step - loss: 15.0239 - accuracy: 0.2214\n","Epoch 21/50\n","66/66 [==============================] - 0s 5ms/step - loss: 10.3309 - accuracy: 0.1993\n","Epoch 22/50\n","66/66 [==============================] - 0s 4ms/step - loss: 6.4451 - accuracy: 0.2128\n","Epoch 23/50\n","66/66 [==============================] - 0s 5ms/step - loss: 4.4833 - accuracy: 0.1974\n","Epoch 24/50\n","66/66 [==============================] - 0s 4ms/step - loss: 2.7241 - accuracy: 0.1960\n","Epoch 25/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.7119 - accuracy: 0.2137\n","Epoch 26/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6244 - accuracy: 0.2248\n","Epoch 27/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6126 - accuracy: 0.2310\n","Epoch 28/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.5980 - accuracy: 0.2512\n","Epoch 29/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6028 - accuracy: 0.2541\n","Epoch 30/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6056 - accuracy: 0.2459\n","Epoch 31/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6079 - accuracy: 0.2531\n","Epoch 32/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6070 - accuracy: 0.2440\n","Epoch 33/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6078 - accuracy: 0.2373\n","Epoch 34/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6035 - accuracy: 0.2296\n","Epoch 35/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6050 - accuracy: 0.2642\n","Epoch 36/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6075 - accuracy: 0.2498\n","Epoch 37/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6068 - accuracy: 0.2512\n","Epoch 38/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6067 - accuracy: 0.2546\n","Epoch 39/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6058 - accuracy: 0.2574\n","Epoch 40/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6085 - accuracy: 0.2478\n","Epoch 41/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6052 - accuracy: 0.2608\n","Epoch 42/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6053 - accuracy: 0.2570\n","Epoch 43/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6046 - accuracy: 0.2656\n","Epoch 44/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6002 - accuracy: 0.2570\n","Epoch 45/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6082 - accuracy: 0.2464\n","Epoch 46/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6065 - accuracy: 0.2478\n","Epoch 47/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6030 - accuracy: 0.2541\n","Epoch 48/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6060 - accuracy: 0.2464\n","Epoch 49/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6068 - accuracy: 0.2560\n","Epoch 50/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6044 - accuracy: 0.2589\n","17/17 [==============================] - 0s 3ms/step\n","Accuracy, Precision, and Recall for all features\n","Accuracy: 0.2514395393474088\n","Precision: 0.4269613868031044\n","Recall: 0.2514395393474088\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["66/66 [==============================] - 2s 5ms/step - loss: 307.8507 - accuracy: 0.1897\n","Epoch 2/50\n","66/66 [==============================] - 0s 5ms/step - loss: 249.2070 - accuracy: 0.1825\n","Epoch 3/50\n","66/66 [==============================] - 0s 5ms/step - loss: 195.8452 - accuracy: 0.1945\n","Epoch 4/50\n","66/66 [==============================] - 0s 6ms/step - loss: 166.6598 - accuracy: 0.1897\n","Epoch 5/50\n","66/66 [==============================] - 1s 18ms/step - loss: 136.4223 - accuracy: 0.2037\n","Epoch 6/50\n","66/66 [==============================] - 0s 5ms/step - loss: 124.0111 - accuracy: 0.1974\n","Epoch 7/50\n","66/66 [==============================] - 0s 3ms/step - loss: 104.6644 - accuracy: 0.2041\n","Epoch 8/50\n","66/66 [==============================] - 0s 3ms/step - loss: 89.7230 - accuracy: 0.2017\n","Epoch 9/50\n","66/66 [==============================] - 0s 3ms/step - loss: 80.7325 - accuracy: 0.1916\n","Epoch 10/50\n","66/66 [==============================] - 0s 3ms/step - loss: 70.4274 - accuracy: 0.1820\n","Epoch 11/50\n","66/66 [==============================] - 0s 3ms/step - loss: 55.4396 - accuracy: 0.1897\n","Epoch 12/50\n","66/66 [==============================] - 0s 3ms/step - loss: 46.5131 - accuracy: 0.2037\n","Epoch 13/50\n","66/66 [==============================] - 0s 3ms/step - loss: 39.1904 - accuracy: 0.1772\n","Epoch 14/50\n","66/66 [==============================] - 0s 3ms/step - loss: 30.3737 - accuracy: 0.2142\n","Epoch 15/50\n","66/66 [==============================] - 0s 3ms/step - loss: 24.3862 - accuracy: 0.2046\n","Epoch 16/50\n","66/66 [==============================] - 0s 4ms/step - loss: 17.9202 - accuracy: 0.1984\n","Epoch 17/50\n","66/66 [==============================] - 0s 5ms/step - loss: 12.7035 - accuracy: 0.2027\n","Epoch 18/50\n","66/66 [==============================] - 0s 8ms/step - loss: 8.3452 - accuracy: 0.2041\n","Epoch 19/50\n","66/66 [==============================] - 0s 6ms/step - loss: 4.5852 - accuracy: 0.2041\n","Epoch 20/50\n","66/66 [==============================] - 0s 3ms/step - loss: 2.1634 - accuracy: 0.1892\n","Epoch 21/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6679 - accuracy: 0.1984\n","Epoch 22/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6582 - accuracy: 0.1816\n","Epoch 23/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6445 - accuracy: 0.1883\n","Epoch 24/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6448 - accuracy: 0.1772\n","Epoch 25/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6339 - accuracy: 0.2065\n","Epoch 26/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6376 - accuracy: 0.1998\n","Epoch 27/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6262 - accuracy: 0.2128\n","Epoch 28/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6269 - accuracy: 0.2219\n","Epoch 29/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6273 - accuracy: 0.2229\n","Epoch 30/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6304 - accuracy: 0.2171\n","Epoch 31/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6162 - accuracy: 0.2315\n","Epoch 32/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6223 - accuracy: 0.2416\n","Epoch 33/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6260 - accuracy: 0.2133\n","Epoch 34/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6202 - accuracy: 0.2291\n","Epoch 35/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6299 - accuracy: 0.2305\n","Epoch 36/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6187 - accuracy: 0.2344\n","Epoch 37/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6194 - accuracy: 0.2416\n","Epoch 38/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6167 - accuracy: 0.2402\n","Epoch 39/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6162 - accuracy: 0.2320\n","Epoch 40/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6127 - accuracy: 0.2296\n","Epoch 41/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6177 - accuracy: 0.2320\n","Epoch 42/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6175 - accuracy: 0.2406\n","Epoch 43/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6152 - accuracy: 0.2378\n","Epoch 44/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6171 - accuracy: 0.2301\n","Epoch 45/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6105 - accuracy: 0.2426\n","Epoch 46/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6189 - accuracy: 0.2349\n","Epoch 47/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6108 - accuracy: 0.2363\n","Epoch 48/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6162 - accuracy: 0.2229\n","Epoch 49/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6151 - accuracy: 0.2243\n","Epoch 50/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6118 - accuracy: 0.2262\n","17/17 [==============================] - 0s 2ms/step\n","Accuracy, Precision, and Recall for all features\n","Accuracy: 0.2380038387715931\n","Precision: 0.8493674310721946\n","Recall: 0.2380038387715931\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["66/66 [==============================] - 2s 4ms/step - loss: 453.8232 - accuracy: 0.2017\n","Epoch 2/50\n","66/66 [==============================] - 0s 5ms/step - loss: 410.4395 - accuracy: 0.2017\n","Epoch 3/50\n","66/66 [==============================] - 0s 5ms/step - loss: 353.2269 - accuracy: 0.1988\n","Epoch 4/50\n","66/66 [==============================] - 0s 5ms/step - loss: 302.5330 - accuracy: 0.2041\n","Epoch 5/50\n","66/66 [==============================] - 0s 4ms/step - loss: 276.0893 - accuracy: 0.1955\n","Epoch 6/50\n","66/66 [==============================] - 0s 4ms/step - loss: 239.5616 - accuracy: 0.1916\n","Epoch 7/50\n","66/66 [==============================] - 0s 5ms/step - loss: 208.7741 - accuracy: 0.1792\n","Epoch 8/50\n","66/66 [==============================] - 0s 5ms/step - loss: 170.2783 - accuracy: 0.2041\n","Epoch 9/50\n","66/66 [==============================] - 0s 5ms/step - loss: 153.9999 - accuracy: 0.1888\n","Epoch 10/50\n","66/66 [==============================] - 0s 5ms/step - loss: 119.6742 - accuracy: 0.2008\n","Epoch 11/50\n","66/66 [==============================] - 0s 5ms/step - loss: 102.3402 - accuracy: 0.2070\n","Epoch 12/50\n","66/66 [==============================] - 0s 5ms/step - loss: 86.4085 - accuracy: 0.1960\n","Epoch 13/50\n","66/66 [==============================] - 0s 5ms/step - loss: 70.0452 - accuracy: 0.1912\n","Epoch 14/50\n","66/66 [==============================] - 0s 5ms/step - loss: 56.8641 - accuracy: 0.1969\n","Epoch 15/50\n","66/66 [==============================] - 0s 4ms/step - loss: 45.3355 - accuracy: 0.1912\n","Epoch 16/50\n","66/66 [==============================] - 0s 5ms/step - loss: 34.3673 - accuracy: 0.2037\n","Epoch 17/50\n","66/66 [==============================] - 0s 5ms/step - loss: 26.2952 - accuracy: 0.2032\n","Epoch 18/50\n","66/66 [==============================] - 0s 3ms/step - loss: 18.4839 - accuracy: 0.2099\n","Epoch 19/50\n","66/66 [==============================] - 0s 3ms/step - loss: 13.4919 - accuracy: 0.1830\n","Epoch 20/50\n","66/66 [==============================] - 0s 3ms/step - loss: 8.4040 - accuracy: 0.1940\n","Epoch 21/50\n","66/66 [==============================] - 0s 3ms/step - loss: 4.4339 - accuracy: 0.2046\n","Epoch 22/50\n","66/66 [==============================] - 0s 3ms/step - loss: 2.0930 - accuracy: 0.2022\n","Epoch 23/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6731 - accuracy: 0.2080\n","Epoch 24/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6535 - accuracy: 0.1950\n","Epoch 25/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6403 - accuracy: 0.2022\n","Epoch 26/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6419 - accuracy: 0.2113\n","Epoch 27/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6423 - accuracy: 0.2147\n","Epoch 28/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6499 - accuracy: 0.2094\n","Epoch 29/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6203 - accuracy: 0.2195\n","Epoch 30/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6316 - accuracy: 0.2133\n","Epoch 31/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6346 - accuracy: 0.2219\n","Epoch 32/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6246 - accuracy: 0.2161\n","Epoch 33/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6287 - accuracy: 0.2128\n","Epoch 34/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6349 - accuracy: 0.2161\n","Epoch 35/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6256 - accuracy: 0.2248\n","Epoch 36/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6230 - accuracy: 0.2224\n","Epoch 37/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6180 - accuracy: 0.2176\n","Epoch 38/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6264 - accuracy: 0.2190\n","Epoch 39/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6217 - accuracy: 0.2277\n","Epoch 40/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6267 - accuracy: 0.2205\n","Epoch 41/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6192 - accuracy: 0.2209\n","Epoch 42/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6219 - accuracy: 0.2257\n","Epoch 43/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6163 - accuracy: 0.2272\n","Epoch 44/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6180 - accuracy: 0.2214\n","Epoch 45/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6198 - accuracy: 0.2224\n","Epoch 46/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6124 - accuracy: 0.2257\n","Epoch 47/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6177 - accuracy: 0.2238\n","Epoch 48/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6163 - accuracy: 0.2229\n","Epoch 49/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6132 - accuracy: 0.2291\n","Epoch 50/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6206 - accuracy: 0.2257\n","17/17 [==============================] - 0s 2ms/step\n","Accuracy, Precision, and Recall for all features\n","Accuracy: 0.2380038387715931\n","Precision: 0.9980806142034548\n","Recall: 0.2380038387715931\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["66/66 [==============================] - 2s 3ms/step - loss: 354.8930 - accuracy: 0.2141\n","Epoch 2/50\n","66/66 [==============================] - 0s 3ms/step - loss: 304.8152 - accuracy: 0.1992\n","Epoch 3/50\n","66/66 [==============================] - 0s 3ms/step - loss: 255.9797 - accuracy: 0.2036\n","Epoch 4/50\n","66/66 [==============================] - 0s 3ms/step - loss: 218.4130 - accuracy: 0.2136\n","Epoch 5/50\n","66/66 [==============================] - 0s 3ms/step - loss: 190.1098 - accuracy: 0.2127\n","Epoch 6/50\n","66/66 [==============================] - 0s 3ms/step - loss: 167.3119 - accuracy: 0.2055\n","Epoch 7/50\n","66/66 [==============================] - 0s 3ms/step - loss: 149.0147 - accuracy: 0.1863\n","Epoch 8/50\n","66/66 [==============================] - 0s 3ms/step - loss: 121.3899 - accuracy: 0.2127\n","Epoch 9/50\n","66/66 [==============================] - 0s 3ms/step - loss: 109.7922 - accuracy: 0.2021\n","Epoch 10/50\n","66/66 [==============================] - 0s 3ms/step - loss: 93.8682 - accuracy: 0.2199\n","Epoch 11/50\n","66/66 [==============================] - 0s 3ms/step - loss: 84.0347 - accuracy: 0.2079\n","Epoch 12/50\n","66/66 [==============================] - 0s 3ms/step - loss: 72.1528 - accuracy: 0.2184\n","Epoch 13/50\n","66/66 [==============================] - 0s 3ms/step - loss: 68.8728 - accuracy: 0.1916\n","Epoch 14/50\n","66/66 [==============================] - 0s 3ms/step - loss: 61.0691 - accuracy: 0.2103\n","Epoch 15/50\n","66/66 [==============================] - 0s 3ms/step - loss: 55.1326 - accuracy: 0.1930\n","Epoch 16/50\n","66/66 [==============================] - 0s 3ms/step - loss: 49.1841 - accuracy: 0.2031\n","Epoch 17/50\n","66/66 [==============================] - 0s 3ms/step - loss: 42.6919 - accuracy: 0.1896\n","Epoch 18/50\n","66/66 [==============================] - 0s 3ms/step - loss: 37.9475 - accuracy: 0.2002\n","Epoch 19/50\n","66/66 [==============================] - 0s 3ms/step - loss: 34.4491 - accuracy: 0.2036\n","Epoch 20/50\n","66/66 [==============================] - 0s 3ms/step - loss: 30.5235 - accuracy: 0.2045\n","Epoch 21/50\n","66/66 [==============================] - 0s 3ms/step - loss: 27.2823 - accuracy: 0.2093\n","Epoch 22/50\n","66/66 [==============================] - 0s 3ms/step - loss: 23.8774 - accuracy: 0.2050\n","Epoch 23/50\n","66/66 [==============================] - 0s 3ms/step - loss: 23.2838 - accuracy: 0.1824\n","Epoch 24/50\n","66/66 [==============================] - 0s 3ms/step - loss: 18.2758 - accuracy: 0.2136\n","Epoch 25/50\n","66/66 [==============================] - 0s 3ms/step - loss: 15.1339 - accuracy: 0.2156\n","Epoch 26/50\n","66/66 [==============================] - 0s 3ms/step - loss: 13.7255 - accuracy: 0.1959\n","Epoch 27/50\n","66/66 [==============================] - 0s 3ms/step - loss: 11.7092 - accuracy: 0.1901\n","Epoch 28/50\n","66/66 [==============================] - 0s 3ms/step - loss: 9.3408 - accuracy: 0.2074\n","Epoch 29/50\n","66/66 [==============================] - 0s 3ms/step - loss: 7.4468 - accuracy: 0.2165\n","Epoch 30/50\n","66/66 [==============================] - 0s 3ms/step - loss: 6.0728 - accuracy: 0.2031\n","Epoch 31/50\n","66/66 [==============================] - 0s 3ms/step - loss: 4.3974 - accuracy: 0.1973\n","Epoch 32/50\n","66/66 [==============================] - 0s 3ms/step - loss: 2.8982 - accuracy: 0.2132\n","Epoch 33/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.9438 - accuracy: 0.1968\n","Epoch 34/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6817 - accuracy: 0.2194\n","Epoch 35/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6361 - accuracy: 0.1992\n","Epoch 36/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6338 - accuracy: 0.2184\n","Epoch 37/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6300 - accuracy: 0.2180\n","Epoch 38/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6260 - accuracy: 0.2271\n","Epoch 39/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6265 - accuracy: 0.2386\n","Epoch 40/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6226 - accuracy: 0.2309\n","Epoch 41/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6242 - accuracy: 0.2357\n","Epoch 42/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6223 - accuracy: 0.2328\n","Epoch 43/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6251 - accuracy: 0.2261\n","Epoch 44/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6228 - accuracy: 0.2343\n","Epoch 45/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6165 - accuracy: 0.2444\n","Epoch 46/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6202 - accuracy: 0.2324\n","Epoch 47/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6140 - accuracy: 0.2348\n","Epoch 48/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6156 - accuracy: 0.2319\n","Epoch 49/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6205 - accuracy: 0.2276\n","Epoch 50/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6135 - accuracy: 0.2338\n","17/17 [==============================] - 0s 3ms/step\n","Accuracy, Precision, and Recall for all features\n","Accuracy: 0.19038461538461537\n","Precision: 1.0\n","Recall: 0.19038461538461537\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["66/66 [==============================] - 2s 5ms/step - loss: 302.3443 - accuracy: 0.1757\n","Epoch 2/50\n","66/66 [==============================] - 0s 5ms/step - loss: 250.5031 - accuracy: 0.1882\n","Epoch 3/50\n","66/66 [==============================] - 0s 4ms/step - loss: 197.9112 - accuracy: 0.1925\n","Epoch 4/50\n","66/66 [==============================] - 0s 3ms/step - loss: 172.3871 - accuracy: 0.1819\n","Epoch 5/50\n","66/66 [==============================] - 0s 3ms/step - loss: 148.2324 - accuracy: 0.1940\n","Epoch 6/50\n","66/66 [==============================] - 0s 3ms/step - loss: 129.5315 - accuracy: 0.1877\n","Epoch 7/50\n","66/66 [==============================] - 0s 3ms/step - loss: 104.4066 - accuracy: 0.2074\n","Epoch 8/50\n","66/66 [==============================] - 0s 3ms/step - loss: 93.0077 - accuracy: 0.1978\n","Epoch 9/50\n","66/66 [==============================] - 0s 3ms/step - loss: 83.3355 - accuracy: 0.1973\n","Epoch 10/50\n","66/66 [==============================] - 0s 3ms/step - loss: 73.4644 - accuracy: 0.1930\n","Epoch 11/50\n","66/66 [==============================] - 0s 3ms/step - loss: 63.8181 - accuracy: 0.1964\n","Epoch 12/50\n","66/66 [==============================] - 0s 3ms/step - loss: 59.3251 - accuracy: 0.1978\n","Epoch 13/50\n","66/66 [==============================] - 0s 3ms/step - loss: 51.6348 - accuracy: 0.1771\n","Epoch 14/50\n","66/66 [==============================] - 0s 3ms/step - loss: 44.0266 - accuracy: 0.1853\n","Epoch 15/50\n","66/66 [==============================] - 0s 3ms/step - loss: 38.2783 - accuracy: 0.2088\n","Epoch 16/50\n","66/66 [==============================] - 0s 3ms/step - loss: 31.6926 - accuracy: 0.1896\n","Epoch 17/50\n","66/66 [==============================] - 0s 3ms/step - loss: 27.2246 - accuracy: 0.1738\n","Epoch 18/50\n","66/66 [==============================] - 0s 3ms/step - loss: 22.1815 - accuracy: 0.2055\n","Epoch 19/50\n","66/66 [==============================] - 0s 3ms/step - loss: 17.4398 - accuracy: 0.1848\n","Epoch 20/50\n","66/66 [==============================] - 0s 3ms/step - loss: 12.7596 - accuracy: 0.1872\n","Epoch 21/50\n","66/66 [==============================] - 0s 3ms/step - loss: 8.2510 - accuracy: 0.1930\n","Epoch 22/50\n","66/66 [==============================] - 0s 3ms/step - loss: 4.0896 - accuracy: 0.1882\n","Epoch 23/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.7983 - accuracy: 0.1887\n","Epoch 24/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6410 - accuracy: 0.2060\n","Epoch 25/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6280 - accuracy: 0.2252\n","Epoch 26/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6366 - accuracy: 0.2285\n","Epoch 27/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6319 - accuracy: 0.2184\n","Epoch 28/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6237 - accuracy: 0.2184\n","Epoch 29/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6267 - accuracy: 0.2194\n","Epoch 30/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6261 - accuracy: 0.2223\n","Epoch 31/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6171 - accuracy: 0.2237\n","Epoch 32/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6167 - accuracy: 0.2132\n","Epoch 33/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6136 - accuracy: 0.2482\n","Epoch 34/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6151 - accuracy: 0.2367\n","Epoch 35/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6128 - accuracy: 0.2256\n","Epoch 36/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6118 - accuracy: 0.2261\n","Epoch 37/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6105 - accuracy: 0.2396\n","Epoch 38/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6106 - accuracy: 0.2444\n","Epoch 39/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6131 - accuracy: 0.2276\n","Epoch 40/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6131 - accuracy: 0.2261\n","Epoch 41/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6134 - accuracy: 0.2357\n","Epoch 42/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6123 - accuracy: 0.2511\n","Epoch 43/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6127 - accuracy: 0.2343\n","Epoch 44/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6133 - accuracy: 0.2444\n","Epoch 45/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6097 - accuracy: 0.2376\n","Epoch 46/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6139 - accuracy: 0.2280\n","Epoch 47/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6144 - accuracy: 0.2362\n","Epoch 48/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6073 - accuracy: 0.2626\n","Epoch 49/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6065 - accuracy: 0.2400\n","Epoch 50/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6152 - accuracy: 0.2415\n","17/17 [==============================] - 0s 2ms/step\n","Accuracy, Precision, and Recall for all features\n","Accuracy: 0.2403846153846154\n","Precision: 0.951007326007326\n","Recall: 0.2403846153846154\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","metadata":{"id":"KDlP3FDydD9X","executionInfo":{"status":"ok","timestamp":1701182930896,"user_tz":-420,"elapsed":13871,"user":{"displayName":"Nurul Andini M277BSX1256","userId":"07500154941621038947"}}},"source":["classifier = RandomForestClassifier(n_estimators = 1500, random_state = 42, n_jobs = -1)\n","classifier.fit(data_train, label_train)\n","feat_labels = dataset.columns[:-1]\n","importances = classifier.feature_importances_\n","indices = np.argsort(importances)[::-1]"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZX8udOgYdD9Z","colab":{"base_uri":"https://localhost:8080/","height":299},"executionInfo":{"status":"ok","timestamp":1701182931724,"user_tz":-420,"elapsed":854,"user":{"displayName":"Nurul Andini M277BSX1256","userId":"07500154941621038947"}},"outputId":"caad0c46-b503-4214-a5a7-d9098723f5c0"},"source":["plt.figure(figsize = (12, 4))\n","plt.title('Feature Importances')\n","plt.bar(range(data_train.shape[1]), importances[indices], color='lightblue', align='center')\n","plt.xticks(range(data_train.shape[1]), feat_labels[indices], rotation=90)\n","plt.xlim([-1, data_train.shape[1]])\n","plt.tight_layout()\n","plt.show()"],"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1200x400 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"G21vYS4JdD9a","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1701183283859,"user_tz":-420,"elapsed":352162,"user":{"displayName":"Nurul Andini M277BSX1256","userId":"07500154941621038947"}},"outputId":"efa3a4fa-d5d6-48f3-a48d-bddf59077960"},"source":["threshold = 40\n","total_features = len(feat_labels) - 1\n","for num_features in range(total_features, threshold, -1):\n","    new_dataset = dataset.drop(columns = feat_labels[indices[num_features:]])\n","    new_data = new_dataset[feat_labels[indices[:num_features]]].values\n","    new_labels = new_dataset['major'].values\n","    new_labels = new_labels.reshape((1, new_labels.shape[0]))\n","    new_labels = transformer.fit_transform(new_labels.T)\n","    for train_index, test_index in k_fold.split(new_data):\n","        data_train, data_test = new_data[train_index], new_data[test_index]\n","        label_train, label_test = labels[train_index], labels[test_index]\n","        model = create_model(input_shape = (num_features, ))\n","        history = model.fit(data_train, label_train, epochs = 50, batch_size = 32)\n","        acc, prec, rec = predict_assess(model, data_test, label_test)\n","        training_report(acc, prec, rec, threshold)\n","        add_to_history(training_history, threshold, acc, prec, rec)"],"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["66/66 [==============================] - 1s 3ms/step - loss: 409.8941 - accuracy: 0.1945\n","Epoch 2/50\n","66/66 [==============================] - 0s 3ms/step - loss: 326.0158 - accuracy: 0.1955\n","Epoch 3/50\n","66/66 [==============================] - 0s 3ms/step - loss: 275.4792 - accuracy: 0.1955\n","Epoch 4/50\n","66/66 [==============================] - 0s 3ms/step - loss: 227.3491 - accuracy: 0.2190\n","Epoch 5/50\n","66/66 [==============================] - 0s 3ms/step - loss: 194.8692 - accuracy: 0.2041\n","Epoch 6/50\n","66/66 [==============================] - 0s 3ms/step - loss: 163.1930 - accuracy: 0.1979\n","Epoch 7/50\n","66/66 [==============================] - 0s 3ms/step - loss: 129.9244 - accuracy: 0.2152\n","Epoch 8/50\n","66/66 [==============================] - 0s 3ms/step - loss: 108.4335 - accuracy: 0.2181\n","Epoch 9/50\n","66/66 [==============================] - 0s 3ms/step - loss: 89.0808 - accuracy: 0.2027\n","Epoch 10/50\n","66/66 [==============================] - 0s 3ms/step - loss: 73.5573 - accuracy: 0.1926\n","Epoch 11/50\n","66/66 [==============================] - 0s 3ms/step - loss: 59.0853 - accuracy: 0.2037\n","Epoch 12/50\n","66/66 [==============================] - 0s 3ms/step - loss: 44.7988 - accuracy: 0.2051\n","Epoch 13/50\n","66/66 [==============================] - 0s 3ms/step - loss: 36.2656 - accuracy: 0.2181\n","Epoch 14/50\n","66/66 [==============================] - 0s 3ms/step - loss: 25.5547 - accuracy: 0.2166\n","Epoch 15/50\n","66/66 [==============================] - 0s 3ms/step - loss: 17.8923 - accuracy: 0.1993\n","Epoch 16/50\n","66/66 [==============================] - 0s 3ms/step - loss: 9.8422 - accuracy: 0.2099\n","Epoch 17/50\n","66/66 [==============================] - 0s 3ms/step - loss: 3.6104 - accuracy: 0.2070\n","Epoch 18/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6411 - accuracy: 0.2262\n","Epoch 19/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6146 - accuracy: 0.2118\n","Epoch 20/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6110 - accuracy: 0.2325\n","Epoch 21/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6101 - accuracy: 0.2344\n","Epoch 22/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6095 - accuracy: 0.2421\n","Epoch 23/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6090 - accuracy: 0.2402\n","Epoch 24/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6093 - accuracy: 0.2320\n","Epoch 25/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6100 - accuracy: 0.2382\n","Epoch 26/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6087 - accuracy: 0.2435\n","Epoch 27/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6100 - accuracy: 0.2392\n","Epoch 28/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6091 - accuracy: 0.2411\n","Epoch 29/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6093 - accuracy: 0.2363\n","Epoch 30/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6092 - accuracy: 0.2363\n","Epoch 31/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6095 - accuracy: 0.2339\n","Epoch 32/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6088 - accuracy: 0.2378\n","Epoch 33/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6081 - accuracy: 0.2435\n","Epoch 34/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6083 - accuracy: 0.2373\n","Epoch 35/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6094 - accuracy: 0.2339\n","Epoch 36/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6087 - accuracy: 0.2368\n","Epoch 37/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6084 - accuracy: 0.2445\n","Epoch 38/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6083 - accuracy: 0.2416\n","Epoch 39/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6081 - accuracy: 0.2416\n","Epoch 40/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6088 - accuracy: 0.2358\n","Epoch 41/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6076 - accuracy: 0.2454\n","Epoch 42/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6089 - accuracy: 0.2387\n","Epoch 43/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6084 - accuracy: 0.2440\n","Epoch 44/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6079 - accuracy: 0.2416\n","Epoch 45/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6084 - accuracy: 0.2402\n","Epoch 46/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6086 - accuracy: 0.2416\n","Epoch 47/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6082 - accuracy: 0.2445\n","Epoch 48/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6077 - accuracy: 0.2430\n","Epoch 49/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6088 - accuracy: 0.2368\n","Epoch 50/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6076 - accuracy: 0.2426\n","17/17 [==============================] - 0s 2ms/step\n","Accuracy, Precision, and Recall for 40 features\n","Accuracy: 0.1727447216890595\n","Precision: 0.19213880131836067\n","Recall: 0.1727447216890595\n","Epoch 1/50\n","66/66 [==============================] - 1s 3ms/step - loss: 339.6381 - accuracy: 0.2085\n","Epoch 2/50\n","66/66 [==============================] - 1s 11ms/step - loss: 279.0825 - accuracy: 0.2109\n","Epoch 3/50\n","66/66 [==============================] - 1s 8ms/step - loss: 227.5228 - accuracy: 0.2085\n","Epoch 4/50\n","66/66 [==============================] - 0s 4ms/step - loss: 187.3457 - accuracy: 0.1878\n","Epoch 5/50\n","66/66 [==============================] - 0s 5ms/step - loss: 150.8006 - accuracy: 0.2094\n","Epoch 6/50\n","66/66 [==============================] - 0s 5ms/step - loss: 127.1379 - accuracy: 0.1844\n","Epoch 7/50\n","66/66 [==============================] - 0s 5ms/step - loss: 94.8296 - accuracy: 0.1969\n","Epoch 8/50\n","66/66 [==============================] - 0s 4ms/step - loss: 76.3890 - accuracy: 0.1960\n","Epoch 9/50\n","66/66 [==============================] - 0s 4ms/step - loss: 55.7135 - accuracy: 0.1931\n","Epoch 10/50\n","66/66 [==============================] - 0s 5ms/step - loss: 47.5988 - accuracy: 0.1758\n","Epoch 11/50\n","66/66 [==============================] - 0s 5ms/step - loss: 35.2952 - accuracy: 0.1912\n","Epoch 12/50\n","66/66 [==============================] - 0s 4ms/step - loss: 25.3118 - accuracy: 0.1921\n","Epoch 13/50\n","66/66 [==============================] - 0s 4ms/step - loss: 16.3362 - accuracy: 0.1907\n","Epoch 14/50\n","66/66 [==============================] - 0s 4ms/step - loss: 8.7600 - accuracy: 0.1854\n","Epoch 15/50\n","66/66 [==============================] - 0s 4ms/step - loss: 2.6099 - accuracy: 0.2022\n","Epoch 16/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6472 - accuracy: 0.1988\n","Epoch 17/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6423 - accuracy: 0.2080\n","Epoch 18/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6320 - accuracy: 0.2190\n","Epoch 19/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6283 - accuracy: 0.2166\n","Epoch 20/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6419 - accuracy: 0.2195\n","Epoch 21/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6396 - accuracy: 0.2094\n","Epoch 22/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6306 - accuracy: 0.2123\n","Epoch 23/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6271 - accuracy: 0.2142\n","Epoch 24/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6265 - accuracy: 0.2209\n","Epoch 25/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6245 - accuracy: 0.2219\n","Epoch 26/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6163 - accuracy: 0.2214\n","Epoch 27/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6189 - accuracy: 0.2296\n","Epoch 28/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6250 - accuracy: 0.2195\n","Epoch 29/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6207 - accuracy: 0.2157\n","Epoch 30/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6217 - accuracy: 0.2190\n","Epoch 31/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6195 - accuracy: 0.2209\n","Epoch 32/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6174 - accuracy: 0.2248\n","Epoch 33/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6207 - accuracy: 0.2277\n","Epoch 34/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6155 - accuracy: 0.2248\n","Epoch 35/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6212 - accuracy: 0.2262\n","Epoch 36/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6127 - accuracy: 0.2205\n","Epoch 37/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6193 - accuracy: 0.2137\n","Epoch 38/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6148 - accuracy: 0.2161\n","Epoch 39/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6157 - accuracy: 0.2233\n","Epoch 40/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6162 - accuracy: 0.2200\n","Epoch 41/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6128 - accuracy: 0.2219\n","Epoch 42/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6142 - accuracy: 0.2272\n","Epoch 43/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6132 - accuracy: 0.2200\n","Epoch 44/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6078 - accuracy: 0.2219\n","Epoch 45/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6118 - accuracy: 0.2253\n","Epoch 46/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6099 - accuracy: 0.2305\n","Epoch 47/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6158 - accuracy: 0.2229\n","Epoch 48/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6126 - accuracy: 0.2224\n","Epoch 49/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6086 - accuracy: 0.2296\n","Epoch 50/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6085 - accuracy: 0.2214\n","17/17 [==============================] - 0s 2ms/step\n","Accuracy, Precision, and Recall for 40 features\n","Accuracy: 0.26103646833013433\n","Precision: 0.9590575870688159\n","Recall: 0.26103646833013433\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["66/66 [==============================] - 1s 3ms/step - loss: 488.5096 - accuracy: 0.1825\n","Epoch 2/50\n","66/66 [==============================] - 0s 3ms/step - loss: 393.1729 - accuracy: 0.1940\n","Epoch 3/50\n","66/66 [==============================] - 0s 3ms/step - loss: 321.4138 - accuracy: 0.1955\n","Epoch 4/50\n","66/66 [==============================] - 0s 3ms/step - loss: 295.4818 - accuracy: 0.1806\n","Epoch 5/50\n","66/66 [==============================] - 0s 3ms/step - loss: 247.0709 - accuracy: 0.1883\n","Epoch 6/50\n","66/66 [==============================] - 0s 3ms/step - loss: 208.4434 - accuracy: 0.1907\n","Epoch 7/50\n","66/66 [==============================] - 0s 3ms/step - loss: 180.7810 - accuracy: 0.1873\n","Epoch 8/50\n","66/66 [==============================] - 0s 3ms/step - loss: 160.0503 - accuracy: 0.1955\n","Epoch 9/50\n","66/66 [==============================] - 0s 3ms/step - loss: 143.0484 - accuracy: 0.2012\n","Epoch 10/50\n","66/66 [==============================] - 0s 3ms/step - loss: 117.4871 - accuracy: 0.1955\n","Epoch 11/50\n","66/66 [==============================] - 0s 3ms/step - loss: 103.8057 - accuracy: 0.1931\n","Epoch 12/50\n","66/66 [==============================] - 0s 3ms/step - loss: 85.5400 - accuracy: 0.1921\n","Epoch 13/50\n","66/66 [==============================] - 0s 3ms/step - loss: 68.5858 - accuracy: 0.1979\n","Epoch 14/50\n","66/66 [==============================] - 0s 3ms/step - loss: 59.6913 - accuracy: 0.2022\n","Epoch 15/50\n","66/66 [==============================] - 0s 3ms/step - loss: 50.0229 - accuracy: 0.2022\n","Epoch 16/50\n","66/66 [==============================] - 0s 3ms/step - loss: 43.6750 - accuracy: 0.1873\n","Epoch 17/50\n","66/66 [==============================] - 0s 3ms/step - loss: 31.1389 - accuracy: 0.1960\n","Epoch 18/50\n","66/66 [==============================] - 0s 3ms/step - loss: 22.8085 - accuracy: 0.1974\n","Epoch 19/50\n","66/66 [==============================] - 0s 4ms/step - loss: 15.5210 - accuracy: 0.1964\n","Epoch 20/50\n","66/66 [==============================] - 0s 5ms/step - loss: 6.4556 - accuracy: 0.1854\n","Epoch 21/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.7663 - accuracy: 0.2080\n","Epoch 22/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6291 - accuracy: 0.2262\n","Epoch 23/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6299 - accuracy: 0.2354\n","Epoch 24/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6240 - accuracy: 0.2368\n","Epoch 25/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6197 - accuracy: 0.2382\n","Epoch 26/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6219 - accuracy: 0.2344\n","Epoch 27/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6203 - accuracy: 0.2349\n","Epoch 28/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6201 - accuracy: 0.2320\n","Epoch 29/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6154 - accuracy: 0.2421\n","Epoch 30/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6122 - accuracy: 0.2416\n","Epoch 31/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6206 - accuracy: 0.2382\n","Epoch 32/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6194 - accuracy: 0.2387\n","Epoch 33/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6161 - accuracy: 0.2320\n","Epoch 34/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6163 - accuracy: 0.2440\n","Epoch 35/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6186 - accuracy: 0.2373\n","Epoch 36/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6173 - accuracy: 0.2315\n","Epoch 37/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6122 - accuracy: 0.2382\n","Epoch 38/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6146 - accuracy: 0.2378\n","Epoch 39/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6117 - accuracy: 0.2406\n","Epoch 40/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6154 - accuracy: 0.2406\n","Epoch 41/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6142 - accuracy: 0.2454\n","Epoch 42/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6132 - accuracy: 0.2325\n","Epoch 43/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6122 - accuracy: 0.2392\n","Epoch 44/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6114 - accuracy: 0.2450\n","Epoch 45/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6160 - accuracy: 0.2339\n","Epoch 46/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6178 - accuracy: 0.2368\n","Epoch 47/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6158 - accuracy: 0.2315\n","Epoch 48/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6127 - accuracy: 0.2402\n","Epoch 49/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6109 - accuracy: 0.2392\n","Epoch 50/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6138 - accuracy: 0.2301\n","17/17 [==============================] - 0s 2ms/step\n","Accuracy, Precision, and Recall for 40 features\n","Accuracy: 0.236084452975048\n","Precision: 0.9900315769921366\n","Recall: 0.236084452975048\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["66/66 [==============================] - 1s 3ms/step - loss: 174.9313 - accuracy: 0.1781\n","Epoch 2/50\n","66/66 [==============================] - 0s 3ms/step - loss: 141.7635 - accuracy: 0.2040\n","Epoch 3/50\n","66/66 [==============================] - 0s 3ms/step - loss: 113.7557 - accuracy: 0.1978\n","Epoch 4/50\n","66/66 [==============================] - 0s 3ms/step - loss: 97.8885 - accuracy: 0.2036\n","Epoch 5/50\n","66/66 [==============================] - 0s 3ms/step - loss: 85.7219 - accuracy: 0.1863\n","Epoch 6/50\n","66/66 [==============================] - 0s 3ms/step - loss: 69.2018 - accuracy: 0.2040\n","Epoch 7/50\n","66/66 [==============================] - 0s 3ms/step - loss: 59.1687 - accuracy: 0.1988\n","Epoch 8/50\n","66/66 [==============================] - 0s 3ms/step - loss: 51.5595 - accuracy: 0.1944\n","Epoch 9/50\n","66/66 [==============================] - 0s 3ms/step - loss: 43.5546 - accuracy: 0.2132\n","Epoch 10/50\n","66/66 [==============================] - 0s 3ms/step - loss: 35.8751 - accuracy: 0.2012\n","Epoch 11/50\n","66/66 [==============================] - 0s 3ms/step - loss: 30.2627 - accuracy: 0.2021\n","Epoch 12/50\n","66/66 [==============================] - 0s 3ms/step - loss: 23.9940 - accuracy: 0.2064\n","Epoch 13/50\n","66/66 [==============================] - 0s 3ms/step - loss: 17.5040 - accuracy: 0.1935\n","Epoch 14/50\n","66/66 [==============================] - 0s 3ms/step - loss: 12.8438 - accuracy: 0.2084\n","Epoch 15/50\n","66/66 [==============================] - 0s 3ms/step - loss: 7.8470 - accuracy: 0.2122\n","Epoch 16/50\n","66/66 [==============================] - 0s 3ms/step - loss: 3.2511 - accuracy: 0.1892\n","Epoch 17/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6618 - accuracy: 0.1992\n","Epoch 18/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6267 - accuracy: 0.2127\n","Epoch 19/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6242 - accuracy: 0.2328\n","Epoch 20/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6197 - accuracy: 0.2324\n","Epoch 21/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6221 - accuracy: 0.2204\n","Epoch 22/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6182 - accuracy: 0.2376\n","Epoch 23/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6165 - accuracy: 0.2285\n","Epoch 24/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6213 - accuracy: 0.2256\n","Epoch 25/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6205 - accuracy: 0.2261\n","Epoch 26/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6183 - accuracy: 0.2367\n","Epoch 27/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6133 - accuracy: 0.2328\n","Epoch 28/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6157 - accuracy: 0.2328\n","Epoch 29/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6136 - accuracy: 0.2343\n","Epoch 30/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6138 - accuracy: 0.2338\n","Epoch 31/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6157 - accuracy: 0.2324\n","Epoch 32/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6178 - accuracy: 0.2372\n","Epoch 33/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6117 - accuracy: 0.2381\n","Epoch 34/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6123 - accuracy: 0.2429\n","Epoch 35/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6146 - accuracy: 0.2420\n","Epoch 36/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6136 - accuracy: 0.2352\n","Epoch 37/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6143 - accuracy: 0.2410\n","Epoch 38/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6106 - accuracy: 0.2444\n","Epoch 39/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6095 - accuracy: 0.2391\n","Epoch 40/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6102 - accuracy: 0.2410\n","Epoch 41/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6127 - accuracy: 0.2352\n","Epoch 42/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6115 - accuracy: 0.2420\n","Epoch 43/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6137 - accuracy: 0.2391\n","Epoch 44/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6135 - accuracy: 0.2429\n","Epoch 45/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6099 - accuracy: 0.2396\n","Epoch 46/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6114 - accuracy: 0.2424\n","Epoch 47/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6086 - accuracy: 0.2410\n","Epoch 48/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6093 - accuracy: 0.2424\n","Epoch 49/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6112 - accuracy: 0.2405\n","Epoch 50/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6099 - accuracy: 0.2482\n","17/17 [==============================] - 0s 3ms/step\n","Accuracy, Precision, and Recall for 40 features\n","Accuracy: 0.19038461538461537\n","Precision: 0.9980769230769231\n","Recall: 0.19038461538461537\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["66/66 [==============================] - 2s 3ms/step - loss: 313.3123 - accuracy: 0.2088\n","Epoch 2/50\n","66/66 [==============================] - 0s 3ms/step - loss: 263.9339 - accuracy: 0.2040\n","Epoch 3/50\n","66/66 [==============================] - 0s 3ms/step - loss: 226.9919 - accuracy: 0.1949\n","Epoch 4/50\n","66/66 [==============================] - 0s 3ms/step - loss: 188.8447 - accuracy: 0.2016\n","Epoch 5/50\n","66/66 [==============================] - 0s 3ms/step - loss: 152.3739 - accuracy: 0.2093\n","Epoch 6/50\n","66/66 [==============================] - 0s 3ms/step - loss: 130.7963 - accuracy: 0.1911\n","Epoch 7/50\n","66/66 [==============================] - 0s 3ms/step - loss: 118.6064 - accuracy: 0.1983\n","Epoch 8/50\n","66/66 [==============================] - 0s 3ms/step - loss: 99.8412 - accuracy: 0.2117\n","Epoch 9/50\n","66/66 [==============================] - 0s 3ms/step - loss: 89.2181 - accuracy: 0.2103\n","Epoch 10/50\n","66/66 [==============================] - 0s 3ms/step - loss: 75.0672 - accuracy: 0.2021\n","Epoch 11/50\n","66/66 [==============================] - 0s 3ms/step - loss: 68.7352 - accuracy: 0.1973\n","Epoch 12/50\n","66/66 [==============================] - 0s 3ms/step - loss: 65.4785 - accuracy: 0.1925\n","Epoch 13/50\n","66/66 [==============================] - 0s 3ms/step - loss: 55.9333 - accuracy: 0.1896\n","Epoch 14/50\n","66/66 [==============================] - 0s 3ms/step - loss: 53.3316 - accuracy: 0.1997\n","Epoch 15/50\n","66/66 [==============================] - 0s 3ms/step - loss: 47.6298 - accuracy: 0.2074\n","Epoch 16/50\n","66/66 [==============================] - 0s 4ms/step - loss: 43.6663 - accuracy: 0.2079\n","Epoch 17/50\n","66/66 [==============================] - 0s 3ms/step - loss: 38.2452 - accuracy: 0.1988\n","Epoch 18/50\n","66/66 [==============================] - 0s 3ms/step - loss: 36.1807 - accuracy: 0.2002\n","Epoch 19/50\n","66/66 [==============================] - 0s 3ms/step - loss: 33.2825 - accuracy: 0.1959\n","Epoch 20/50\n","66/66 [==============================] - 0s 3ms/step - loss: 30.2253 - accuracy: 0.2002\n","Epoch 21/50\n","66/66 [==============================] - 0s 3ms/step - loss: 25.8200 - accuracy: 0.1786\n","Epoch 22/50\n","66/66 [==============================] - 0s 3ms/step - loss: 23.2300 - accuracy: 0.2132\n","Epoch 23/50\n","66/66 [==============================] - 0s 3ms/step - loss: 20.7621 - accuracy: 0.1997\n","Epoch 24/50\n","66/66 [==============================] - 0s 3ms/step - loss: 18.9051 - accuracy: 0.1882\n","Epoch 25/50\n","66/66 [==============================] - 0s 3ms/step - loss: 15.6770 - accuracy: 0.1949\n","Epoch 26/50\n","66/66 [==============================] - 0s 3ms/step - loss: 12.4833 - accuracy: 0.2103\n","Epoch 27/50\n","66/66 [==============================] - 0s 3ms/step - loss: 10.1706 - accuracy: 0.1877\n","Epoch 28/50\n","66/66 [==============================] - 0s 3ms/step - loss: 7.2954 - accuracy: 0.1901\n","Epoch 29/50\n","66/66 [==============================] - 0s 3ms/step - loss: 5.0305 - accuracy: 0.2045\n","Epoch 30/50\n","66/66 [==============================] - 0s 3ms/step - loss: 3.0645 - accuracy: 0.2031\n","Epoch 31/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.8937 - accuracy: 0.2218\n","Epoch 32/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6586 - accuracy: 0.2021\n","Epoch 33/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6221 - accuracy: 0.2285\n","Epoch 34/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6256 - accuracy: 0.2333\n","Epoch 35/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6217 - accuracy: 0.2165\n","Epoch 36/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6142 - accuracy: 0.2180\n","Epoch 37/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6201 - accuracy: 0.2127\n","Epoch 38/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6245 - accuracy: 0.2156\n","Epoch 39/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6072 - accuracy: 0.2237\n","Epoch 40/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6156 - accuracy: 0.2295\n","Epoch 41/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6204 - accuracy: 0.2252\n","Epoch 42/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6161 - accuracy: 0.2237\n","Epoch 43/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6122 - accuracy: 0.2319\n","Epoch 44/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6084 - accuracy: 0.2420\n","Epoch 45/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6119 - accuracy: 0.2314\n","Epoch 46/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6131 - accuracy: 0.2266\n","Epoch 47/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6134 - accuracy: 0.2290\n","Epoch 48/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6072 - accuracy: 0.2338\n","Epoch 49/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6140 - accuracy: 0.2242\n","Epoch 50/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6081 - accuracy: 0.2218\n","17/17 [==============================] - 0s 3ms/step\n","Accuracy, Precision, and Recall for 40 features\n","Accuracy: 0.2403846153846154\n","Precision: 0.9766483516483516\n","Recall: 0.2403846153846154\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","66/66 [==============================] - 2s 5ms/step - loss: 832.6610 - accuracy: 0.1873\n","Epoch 2/50\n","66/66 [==============================] - 0s 5ms/step - loss: 670.8768 - accuracy: 0.2089\n","Epoch 3/50\n","66/66 [==============================] - 0s 4ms/step - loss: 565.7504 - accuracy: 0.1936\n","Epoch 4/50\n","66/66 [==============================] - 0s 4ms/step - loss: 482.7301 - accuracy: 0.1950\n","Epoch 5/50\n","66/66 [==============================] - 0s 5ms/step - loss: 397.5895 - accuracy: 0.2041\n","Epoch 6/50\n","66/66 [==============================] - 0s 5ms/step - loss: 339.5748 - accuracy: 0.1993\n","Epoch 7/50\n","66/66 [==============================] - 0s 5ms/step - loss: 293.0008 - accuracy: 0.1988\n","Epoch 8/50\n","66/66 [==============================] - 0s 5ms/step - loss: 263.2677 - accuracy: 0.2003\n","Epoch 9/50\n","66/66 [==============================] - 0s 5ms/step - loss: 226.1728 - accuracy: 0.1974\n","Epoch 10/50\n","66/66 [==============================] - 0s 5ms/step - loss: 197.7027 - accuracy: 0.1988\n","Epoch 11/50\n","66/66 [==============================] - 0s 5ms/step - loss: 172.8063 - accuracy: 0.2075\n","Epoch 12/50\n","66/66 [==============================] - 0s 5ms/step - loss: 144.0455 - accuracy: 0.2123\n","Epoch 13/50\n","66/66 [==============================] - 0s 3ms/step - loss: 131.4024 - accuracy: 0.2137\n","Epoch 14/50\n","66/66 [==============================] - 0s 3ms/step - loss: 110.3394 - accuracy: 0.2032\n","Epoch 15/50\n","66/66 [==============================] - 0s 3ms/step - loss: 93.5043 - accuracy: 0.1936\n","Epoch 16/50\n","66/66 [==============================] - 0s 5ms/step - loss: 80.2588 - accuracy: 0.2171\n","Epoch 17/50\n","66/66 [==============================] - 0s 6ms/step - loss: 66.7573 - accuracy: 0.2046\n","Epoch 18/50\n","66/66 [==============================] - 1s 9ms/step - loss: 52.8210 - accuracy: 0.2113\n","Epoch 19/50\n","66/66 [==============================] - 0s 7ms/step - loss: 42.8996 - accuracy: 0.2142\n","Epoch 20/50\n","66/66 [==============================] - 0s 7ms/step - loss: 31.9423 - accuracy: 0.2075\n","Epoch 21/50\n","66/66 [==============================] - 1s 8ms/step - loss: 23.0703 - accuracy: 0.2137\n","Epoch 22/50\n","66/66 [==============================] - 0s 6ms/step - loss: 15.1709 - accuracy: 0.2238\n","Epoch 23/50\n","66/66 [==============================] - 0s 5ms/step - loss: 8.3200 - accuracy: 0.1979\n","Epoch 24/50\n","66/66 [==============================] - 0s 6ms/step - loss: 2.6296 - accuracy: 0.2032\n","Epoch 25/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6268 - accuracy: 0.1950\n","Epoch 26/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6169 - accuracy: 0.1912\n","Epoch 27/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6091 - accuracy: 0.2089\n","Epoch 28/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6106 - accuracy: 0.2051\n","Epoch 29/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6124 - accuracy: 0.1945\n","Epoch 30/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6099 - accuracy: 0.2301\n","Epoch 31/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6092 - accuracy: 0.2296\n","Epoch 32/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6094 - accuracy: 0.2310\n","Epoch 33/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6088 - accuracy: 0.2281\n","Epoch 34/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6086 - accuracy: 0.2286\n","Epoch 35/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6087 - accuracy: 0.2325\n","Epoch 36/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6088 - accuracy: 0.2339\n","Epoch 37/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6104 - accuracy: 0.2301\n","Epoch 38/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6094 - accuracy: 0.2233\n","Epoch 39/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6081 - accuracy: 0.2334\n","Epoch 40/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6081 - accuracy: 0.2310\n","Epoch 41/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6083 - accuracy: 0.2305\n","Epoch 42/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6068 - accuracy: 0.2339\n","Epoch 43/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6064 - accuracy: 0.2320\n","Epoch 44/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6094 - accuracy: 0.2315\n","Epoch 45/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6051 - accuracy: 0.2262\n","Epoch 46/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6097 - accuracy: 0.2286\n","Epoch 47/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6079 - accuracy: 0.2315\n","Epoch 48/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6070 - accuracy: 0.2349\n","Epoch 49/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6071 - accuracy: 0.2310\n","Epoch 50/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6080 - accuracy: 0.2224\n","17/17 [==============================] - 0s 2ms/step\n","Accuracy, Precision, and Recall for 40 features\n","Accuracy: 0.18042226487523993\n","Precision: 0.5587574422308282\n","Recall: 0.18042226487523993\n","Epoch 1/50\n","66/66 [==============================] - 1s 3ms/step - loss: 256.0184 - accuracy: 0.2065\n","Epoch 2/50\n","66/66 [==============================] - 0s 3ms/step - loss: 215.1892 - accuracy: 0.2051\n","Epoch 3/50\n","66/66 [==============================] - 0s 3ms/step - loss: 159.8275 - accuracy: 0.2176\n","Epoch 4/50\n","66/66 [==============================] - 0s 3ms/step - loss: 138.1695 - accuracy: 0.1921\n","Epoch 5/50\n","66/66 [==============================] - 0s 3ms/step - loss: 118.9219 - accuracy: 0.1892\n","Epoch 6/50\n","66/66 [==============================] - 0s 3ms/step - loss: 96.7775 - accuracy: 0.2075\n","Epoch 7/50\n","66/66 [==============================] - 0s 3ms/step - loss: 80.4918 - accuracy: 0.1988\n","Epoch 8/50\n","66/66 [==============================] - 0s 3ms/step - loss: 63.1473 - accuracy: 0.2080\n","Epoch 9/50\n","66/66 [==============================] - 0s 3ms/step - loss: 52.4297 - accuracy: 0.1806\n","Epoch 10/50\n","66/66 [==============================] - 0s 3ms/step - loss: 42.5287 - accuracy: 0.2085\n","Epoch 11/50\n","66/66 [==============================] - 0s 3ms/step - loss: 34.8559 - accuracy: 0.1964\n","Epoch 12/50\n","66/66 [==============================] - 0s 3ms/step - loss: 25.4499 - accuracy: 0.1969\n","Epoch 13/50\n","66/66 [==============================] - 0s 3ms/step - loss: 15.5867 - accuracy: 0.2123\n","Epoch 14/50\n","66/66 [==============================] - 0s 3ms/step - loss: 8.9171 - accuracy: 0.2137\n","Epoch 15/50\n","66/66 [==============================] - 0s 3ms/step - loss: 3.5094 - accuracy: 0.1974\n","Epoch 16/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6549 - accuracy: 0.2061\n","Epoch 17/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6453 - accuracy: 0.1854\n","Epoch 18/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6345 - accuracy: 0.1888\n","Epoch 19/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6321 - accuracy: 0.1902\n","Epoch 20/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6319 - accuracy: 0.1859\n","Epoch 21/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6272 - accuracy: 0.1864\n","Epoch 22/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6271 - accuracy: 0.1859\n","Epoch 23/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6447 - accuracy: 0.1652\n","Epoch 24/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6359 - accuracy: 0.1676\n","Epoch 25/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6254 - accuracy: 0.1907\n","Epoch 26/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6253 - accuracy: 0.2176\n","Epoch 27/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6268 - accuracy: 0.2147\n","Epoch 28/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6295 - accuracy: 0.2205\n","Epoch 29/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6192 - accuracy: 0.2209\n","Epoch 30/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6191 - accuracy: 0.2176\n","Epoch 31/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6219 - accuracy: 0.2214\n","Epoch 32/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6197 - accuracy: 0.2214\n","Epoch 33/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6152 - accuracy: 0.2286\n","Epoch 34/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6199 - accuracy: 0.2176\n","Epoch 35/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6206 - accuracy: 0.2128\n","Epoch 36/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6207 - accuracy: 0.2195\n","Epoch 37/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6098 - accuracy: 0.2243\n","Epoch 38/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6151 - accuracy: 0.2257\n","Epoch 39/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6165 - accuracy: 0.2358\n","Epoch 40/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6206 - accuracy: 0.2190\n","Epoch 41/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6196 - accuracy: 0.2209\n","Epoch 42/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6194 - accuracy: 0.2185\n","Epoch 43/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6109 - accuracy: 0.2243\n","Epoch 44/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6170 - accuracy: 0.2253\n","Epoch 45/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6140 - accuracy: 0.2262\n","Epoch 46/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6147 - accuracy: 0.2205\n","Epoch 47/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6129 - accuracy: 0.2205\n","Epoch 48/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6165 - accuracy: 0.2243\n","Epoch 49/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6156 - accuracy: 0.2286\n","Epoch 50/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6154 - accuracy: 0.2195\n","17/17 [==============================] - 0s 2ms/step\n","Accuracy, Precision, and Recall for 40 features\n","Accuracy: 0.26103646833013433\n","Precision: 0.9980806142034548\n","Recall: 0.26103646833013433\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["66/66 [==============================] - 2s 3ms/step - loss: 342.5524 - accuracy: 0.2133\n","Epoch 2/50\n","66/66 [==============================] - 0s 3ms/step - loss: 276.8878 - accuracy: 0.1840\n","Epoch 3/50\n","66/66 [==============================] - 0s 3ms/step - loss: 229.1688 - accuracy: 0.1984\n","Epoch 4/50\n","66/66 [==============================] - 0s 3ms/step - loss: 195.0481 - accuracy: 0.2017\n","Epoch 5/50\n","66/66 [==============================] - 0s 3ms/step - loss: 168.7654 - accuracy: 0.1878\n","Epoch 6/50\n","66/66 [==============================] - 0s 3ms/step - loss: 132.0133 - accuracy: 0.1984\n","Epoch 7/50\n","66/66 [==============================] - 0s 3ms/step - loss: 108.9822 - accuracy: 0.2008\n","Epoch 8/50\n","66/66 [==============================] - 0s 3ms/step - loss: 85.6187 - accuracy: 0.1984\n","Epoch 9/50\n","66/66 [==============================] - 0s 3ms/step - loss: 69.9183 - accuracy: 0.2085\n","Epoch 10/50\n","66/66 [==============================] - 0s 3ms/step - loss: 62.2250 - accuracy: 0.1921\n","Epoch 11/50\n","66/66 [==============================] - 0s 3ms/step - loss: 51.6451 - accuracy: 0.1883\n","Epoch 12/50\n","66/66 [==============================] - 0s 3ms/step - loss: 39.3771 - accuracy: 0.1936\n","Epoch 13/50\n","66/66 [==============================] - 0s 3ms/step - loss: 32.0148 - accuracy: 0.1907\n","Epoch 14/50\n","66/66 [==============================] - 0s 3ms/step - loss: 24.1305 - accuracy: 0.1782\n","Epoch 15/50\n","66/66 [==============================] - 0s 3ms/step - loss: 16.5146 - accuracy: 0.1988\n","Epoch 16/50\n","66/66 [==============================] - 0s 3ms/step - loss: 9.8084 - accuracy: 0.1950\n","Epoch 17/50\n","66/66 [==============================] - 0s 3ms/step - loss: 4.2844 - accuracy: 0.1950\n","Epoch 18/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6572 - accuracy: 0.1849\n","Epoch 19/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6209 - accuracy: 0.1897\n","Epoch 20/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6136 - accuracy: 0.1955\n","Epoch 21/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6166 - accuracy: 0.1960\n","Epoch 22/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6146 - accuracy: 0.2037\n","Epoch 23/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6168 - accuracy: 0.2027\n","Epoch 24/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6161 - accuracy: 0.2056\n","Epoch 25/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6178 - accuracy: 0.1964\n","Epoch 26/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6148 - accuracy: 0.1960\n","Epoch 27/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6118 - accuracy: 0.2051\n","Epoch 28/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6124 - accuracy: 0.2104\n","Epoch 29/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6120 - accuracy: 0.1964\n","Epoch 30/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6123 - accuracy: 0.2065\n","Epoch 31/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6104 - accuracy: 0.2334\n","Epoch 32/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6116 - accuracy: 0.2209\n","Epoch 33/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6137 - accuracy: 0.2219\n","Epoch 34/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6127 - accuracy: 0.2267\n","Epoch 35/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6121 - accuracy: 0.2272\n","Epoch 36/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6078 - accuracy: 0.2363\n","Epoch 37/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6164 - accuracy: 0.2339\n","Epoch 38/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6095 - accuracy: 0.2430\n","Epoch 39/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6113 - accuracy: 0.2325\n","Epoch 40/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6142 - accuracy: 0.2382\n","Epoch 41/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6121 - accuracy: 0.2344\n","Epoch 42/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6120 - accuracy: 0.2305\n","Epoch 43/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6096 - accuracy: 0.2373\n","Epoch 44/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6092 - accuracy: 0.2387\n","Epoch 45/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6068 - accuracy: 0.2329\n","Epoch 46/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6128 - accuracy: 0.2315\n","Epoch 47/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6097 - accuracy: 0.2402\n","Epoch 48/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6104 - accuracy: 0.2358\n","Epoch 49/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6074 - accuracy: 0.2334\n","Epoch 50/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6080 - accuracy: 0.2358\n","17/17 [==============================] - 0s 2ms/step\n","Accuracy, Precision, and Recall for 40 features\n","Accuracy: 0.236084452975048\n","Precision: 0.9900315769921366\n","Recall: 0.236084452975048\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["66/66 [==============================] - 2s 5ms/step - loss: 547.2499 - accuracy: 0.1887\n","Epoch 2/50\n","66/66 [==============================] - 0s 4ms/step - loss: 465.3280 - accuracy: 0.1728\n","Epoch 3/50\n","66/66 [==============================] - 0s 4ms/step - loss: 422.9086 - accuracy: 0.1800\n","Epoch 4/50\n","66/66 [==============================] - 0s 4ms/step - loss: 361.8448 - accuracy: 0.1882\n","Epoch 5/50\n","66/66 [==============================] - 0s 5ms/step - loss: 333.8266 - accuracy: 0.1767\n","Epoch 6/50\n","66/66 [==============================] - 0s 4ms/step - loss: 276.5334 - accuracy: 0.1896\n","Epoch 7/50\n","66/66 [==============================] - 0s 5ms/step - loss: 241.2652 - accuracy: 0.1988\n","Epoch 8/50\n","66/66 [==============================] - 0s 5ms/step - loss: 215.6096 - accuracy: 0.1949\n","Epoch 9/50\n","66/66 [==============================] - 0s 5ms/step - loss: 185.8691 - accuracy: 0.1997\n","Epoch 10/50\n","66/66 [==============================] - 0s 5ms/step - loss: 159.7551 - accuracy: 0.1959\n","Epoch 11/50\n","66/66 [==============================] - 0s 5ms/step - loss: 141.5691 - accuracy: 0.1911\n","Epoch 12/50\n","66/66 [==============================] - 0s 5ms/step - loss: 117.4791 - accuracy: 0.2160\n","Epoch 13/50\n","66/66 [==============================] - 0s 5ms/step - loss: 109.0563 - accuracy: 0.1978\n","Epoch 14/50\n","66/66 [==============================] - 0s 4ms/step - loss: 92.0937 - accuracy: 0.1877\n","Epoch 15/50\n","66/66 [==============================] - 0s 3ms/step - loss: 80.8996 - accuracy: 0.2103\n","Epoch 16/50\n","66/66 [==============================] - 0s 3ms/step - loss: 70.2310 - accuracy: 0.1911\n","Epoch 17/50\n","66/66 [==============================] - 0s 3ms/step - loss: 61.5855 - accuracy: 0.2045\n","Epoch 18/50\n","66/66 [==============================] - 0s 3ms/step - loss: 54.2197 - accuracy: 0.2007\n","Epoch 19/50\n","66/66 [==============================] - 0s 3ms/step - loss: 51.6413 - accuracy: 0.1771\n","Epoch 20/50\n","66/66 [==============================] - 0s 3ms/step - loss: 45.4182 - accuracy: 0.1973\n","Epoch 21/50\n","66/66 [==============================] - 0s 3ms/step - loss: 39.2634 - accuracy: 0.2040\n","Epoch 22/50\n","66/66 [==============================] - 0s 3ms/step - loss: 34.2854 - accuracy: 0.1863\n","Epoch 23/50\n","66/66 [==============================] - 0s 3ms/step - loss: 30.3139 - accuracy: 0.1887\n","Epoch 24/50\n","66/66 [==============================] - 0s 3ms/step - loss: 26.1879 - accuracy: 0.1940\n","Epoch 25/50\n","66/66 [==============================] - 0s 3ms/step - loss: 21.4998 - accuracy: 0.1964\n","Epoch 26/50\n","66/66 [==============================] - 0s 3ms/step - loss: 18.0151 - accuracy: 0.1968\n","Epoch 27/50\n","66/66 [==============================] - 0s 3ms/step - loss: 14.6544 - accuracy: 0.1819\n","Epoch 28/50\n","66/66 [==============================] - 0s 3ms/step - loss: 10.7639 - accuracy: 0.1959\n","Epoch 29/50\n","66/66 [==============================] - 0s 3ms/step - loss: 7.4889 - accuracy: 0.1925\n","Epoch 30/50\n","66/66 [==============================] - 0s 3ms/step - loss: 4.8948 - accuracy: 0.1791\n","Epoch 31/50\n","66/66 [==============================] - 0s 3ms/step - loss: 2.5698 - accuracy: 0.2069\n","Epoch 32/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6945 - accuracy: 0.2232\n","Epoch 33/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6557 - accuracy: 0.2232\n","Epoch 34/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6322 - accuracy: 0.2271\n","Epoch 35/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6387 - accuracy: 0.2271\n","Epoch 36/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6328 - accuracy: 0.2280\n","Epoch 37/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6367 - accuracy: 0.2348\n","Epoch 38/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6230 - accuracy: 0.2324\n","Epoch 39/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6215 - accuracy: 0.2376\n","Epoch 40/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6282 - accuracy: 0.2391\n","Epoch 41/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6217 - accuracy: 0.2352\n","Epoch 42/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6226 - accuracy: 0.2362\n","Epoch 43/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6204 - accuracy: 0.2381\n","Epoch 44/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6154 - accuracy: 0.2391\n","Epoch 45/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6185 - accuracy: 0.2463\n","Epoch 46/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6108 - accuracy: 0.2463\n","Epoch 47/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6111 - accuracy: 0.2492\n","Epoch 48/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6166 - accuracy: 0.2444\n","Epoch 49/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6148 - accuracy: 0.2453\n","Epoch 50/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6130 - accuracy: 0.2496\n","17/17 [==============================] - 0s 2ms/step\n","Accuracy, Precision, and Recall for 40 features\n","Accuracy: 0.19230769230769232\n","Precision: 0.9923631656804734\n","Recall: 0.19230769230769232\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["66/66 [==============================] - 1s 3ms/step - loss: 277.9557 - accuracy: 0.2021\n","Epoch 2/50\n","66/66 [==============================] - 0s 3ms/step - loss: 243.0795 - accuracy: 0.2002\n","Epoch 3/50\n","66/66 [==============================] - 0s 3ms/step - loss: 217.9403 - accuracy: 0.1973\n","Epoch 4/50\n","66/66 [==============================] - 0s 3ms/step - loss: 180.9145 - accuracy: 0.1983\n","Epoch 5/50\n","66/66 [==============================] - 0s 3ms/step - loss: 158.4514 - accuracy: 0.2021\n","Epoch 6/50\n","66/66 [==============================] - 0s 3ms/step - loss: 134.7023 - accuracy: 0.1935\n","Epoch 7/50\n","66/66 [==============================] - 0s 4ms/step - loss: 113.8982 - accuracy: 0.2084\n","Epoch 8/50\n","66/66 [==============================] - 0s 4ms/step - loss: 98.5427 - accuracy: 0.1968\n","Epoch 9/50\n","66/66 [==============================] - 0s 4ms/step - loss: 79.1692 - accuracy: 0.2175\n","Epoch 10/50\n","66/66 [==============================] - 0s 5ms/step - loss: 65.6665 - accuracy: 0.1920\n","Epoch 11/50\n","66/66 [==============================] - 0s 5ms/step - loss: 54.8301 - accuracy: 0.2132\n","Epoch 12/50\n","66/66 [==============================] - 0s 5ms/step - loss: 46.6663 - accuracy: 0.1988\n","Epoch 13/50\n","66/66 [==============================] - 0s 5ms/step - loss: 39.6829 - accuracy: 0.2012\n","Epoch 14/50\n","66/66 [==============================] - 0s 4ms/step - loss: 33.7032 - accuracy: 0.1983\n","Epoch 15/50\n","66/66 [==============================] - 0s 5ms/step - loss: 28.9855 - accuracy: 0.1901\n","Epoch 16/50\n","66/66 [==============================] - 0s 5ms/step - loss: 21.7792 - accuracy: 0.1978\n","Epoch 17/50\n","66/66 [==============================] - 0s 5ms/step - loss: 15.9731 - accuracy: 0.1978\n","Epoch 18/50\n","66/66 [==============================] - 0s 5ms/step - loss: 11.5636 - accuracy: 0.1973\n","Epoch 19/50\n","66/66 [==============================] - 0s 5ms/step - loss: 7.5612 - accuracy: 0.2180\n","Epoch 20/50\n","66/66 [==============================] - 0s 4ms/step - loss: 4.6612 - accuracy: 0.1925\n","Epoch 21/50\n","66/66 [==============================] - 0s 4ms/step - loss: 2.3796 - accuracy: 0.2093\n","Epoch 22/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6919 - accuracy: 0.2276\n","Epoch 23/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6320 - accuracy: 0.2386\n","Epoch 24/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6413 - accuracy: 0.2487\n","Epoch 25/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6365 - accuracy: 0.2492\n","Epoch 26/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6289 - accuracy: 0.2453\n","Epoch 27/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6330 - accuracy: 0.2453\n","Epoch 28/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6210 - accuracy: 0.2439\n","Epoch 29/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6213 - accuracy: 0.2444\n","Epoch 30/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6167 - accuracy: 0.2568\n","Epoch 31/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6185 - accuracy: 0.2453\n","Epoch 32/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6189 - accuracy: 0.2626\n","Epoch 33/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6160 - accuracy: 0.2525\n","Epoch 34/50\n","66/66 [==============================] - 0s 6ms/step - loss: 1.6249 - accuracy: 0.2376\n","Epoch 35/50\n","66/66 [==============================] - 1s 9ms/step - loss: 1.6149 - accuracy: 0.2564\n","Epoch 36/50\n","66/66 [==============================] - 0s 7ms/step - loss: 1.6128 - accuracy: 0.2477\n","Epoch 37/50\n","66/66 [==============================] - 0s 7ms/step - loss: 1.6203 - accuracy: 0.2492\n","Epoch 38/50\n","66/66 [==============================] - 0s 7ms/step - loss: 1.6122 - accuracy: 0.2496\n","Epoch 39/50\n","66/66 [==============================] - 1s 8ms/step - loss: 1.6094 - accuracy: 0.2511\n","Epoch 40/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6153 - accuracy: 0.2578\n","Epoch 41/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6208 - accuracy: 0.2554\n","Epoch 42/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6148 - accuracy: 0.2568\n","Epoch 43/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6141 - accuracy: 0.2511\n","Epoch 44/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.5998 - accuracy: 0.2727\n","Epoch 45/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6068 - accuracy: 0.2544\n","Epoch 46/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6097 - accuracy: 0.2592\n","Epoch 47/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6149 - accuracy: 0.2448\n","Epoch 48/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6152 - accuracy: 0.2492\n","Epoch 49/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6160 - accuracy: 0.2472\n","Epoch 50/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6144 - accuracy: 0.2626\n","17/17 [==============================] - 0s 2ms/step\n","Accuracy, Precision, and Recall for 40 features\n","Accuracy: 0.225\n","Precision: 1.0\n","Recall: 0.225\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["66/66 [==============================] - 1s 3ms/step - loss: 484.6243 - accuracy: 0.2152\n","Epoch 2/50\n","66/66 [==============================] - 0s 3ms/step - loss: 415.2216 - accuracy: 0.1988\n","Epoch 3/50\n","66/66 [==============================] - 0s 3ms/step - loss: 358.7729 - accuracy: 0.1998\n","Epoch 4/50\n","66/66 [==============================] - 0s 3ms/step - loss: 307.2752 - accuracy: 0.2032\n","Epoch 5/50\n","66/66 [==============================] - 0s 3ms/step - loss: 253.8726 - accuracy: 0.1912\n","Epoch 6/50\n","66/66 [==============================] - 0s 3ms/step - loss: 221.2459 - accuracy: 0.2262\n","Epoch 7/50\n","66/66 [==============================] - 0s 3ms/step - loss: 183.3878 - accuracy: 0.1940\n","Epoch 8/50\n","66/66 [==============================] - 0s 3ms/step - loss: 160.5002 - accuracy: 0.1940\n","Epoch 9/50\n","66/66 [==============================] - 0s 3ms/step - loss: 123.5369 - accuracy: 0.1945\n","Epoch 10/50\n","66/66 [==============================] - 0s 3ms/step - loss: 104.6335 - accuracy: 0.1998\n","Epoch 11/50\n","66/66 [==============================] - 0s 3ms/step - loss: 83.6136 - accuracy: 0.1936\n","Epoch 12/50\n","66/66 [==============================] - 0s 4ms/step - loss: 67.2558 - accuracy: 0.2037\n","Epoch 13/50\n","66/66 [==============================] - 0s 4ms/step - loss: 52.0297 - accuracy: 0.2008\n","Epoch 14/50\n","66/66 [==============================] - 0s 5ms/step - loss: 38.9247 - accuracy: 0.1940\n","Epoch 15/50\n","66/66 [==============================] - 0s 5ms/step - loss: 27.8714 - accuracy: 0.2099\n","Epoch 16/50\n","66/66 [==============================] - 0s 4ms/step - loss: 22.2614 - accuracy: 0.2070\n","Epoch 17/50\n","66/66 [==============================] - 0s 5ms/step - loss: 18.7441 - accuracy: 0.1902\n","Epoch 18/50\n","66/66 [==============================] - 0s 5ms/step - loss: 13.2950 - accuracy: 0.2041\n","Epoch 19/50\n","66/66 [==============================] - 0s 5ms/step - loss: 10.2837 - accuracy: 0.1998\n","Epoch 20/50\n","66/66 [==============================] - 0s 5ms/step - loss: 7.3396 - accuracy: 0.2003\n","Epoch 21/50\n","66/66 [==============================] - 0s 5ms/step - loss: 4.5692 - accuracy: 0.2161\n","Epoch 22/50\n","66/66 [==============================] - 0s 4ms/step - loss: 2.8079 - accuracy: 0.1950\n","Epoch 23/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.7230 - accuracy: 0.2027\n","Epoch 24/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6311 - accuracy: 0.1883\n","Epoch 25/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6150 - accuracy: 0.1801\n","Epoch 26/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6188 - accuracy: 0.1772\n","Epoch 27/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6098 - accuracy: 0.1763\n","Epoch 28/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6116 - accuracy: 0.1686\n","Epoch 29/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6081 - accuracy: 0.1753\n","Epoch 30/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6112 - accuracy: 0.1715\n","Epoch 31/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6101 - accuracy: 0.1700\n","Epoch 32/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6088 - accuracy: 0.1758\n","Epoch 33/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6073 - accuracy: 0.1763\n","Epoch 34/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6093 - accuracy: 0.1811\n","Epoch 35/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6092 - accuracy: 0.2065\n","Epoch 36/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6098 - accuracy: 0.2248\n","Epoch 37/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6081 - accuracy: 0.2224\n","Epoch 38/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6094 - accuracy: 0.2272\n","Epoch 39/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6083 - accuracy: 0.2373\n","Epoch 40/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6085 - accuracy: 0.2238\n","Epoch 41/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6091 - accuracy: 0.2382\n","Epoch 42/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6070 - accuracy: 0.2349\n","Epoch 43/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6082 - accuracy: 0.2243\n","Epoch 44/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6086 - accuracy: 0.2200\n","Epoch 45/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6103 - accuracy: 0.2291\n","Epoch 46/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6088 - accuracy: 0.2459\n","Epoch 47/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6094 - accuracy: 0.2142\n","Epoch 48/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6074 - accuracy: 0.2392\n","Epoch 49/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6100 - accuracy: 0.2305\n","Epoch 50/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6087 - accuracy: 0.2450\n","17/17 [==============================] - 0s 3ms/step\n","Accuracy, Precision, and Recall for 40 features\n","Accuracy: 0.2053742802303263\n","Precision: 0.43591544770071367\n","Recall: 0.2053742802303263\n","Epoch 1/50\n","66/66 [==============================] - 2s 5ms/step - loss: 294.2329 - accuracy: 0.2104\n","Epoch 2/50\n","66/66 [==============================] - 0s 5ms/step - loss: 261.6723 - accuracy: 0.1969\n","Epoch 3/50\n","66/66 [==============================] - 0s 5ms/step - loss: 207.1048 - accuracy: 0.1969\n","Epoch 4/50\n","66/66 [==============================] - 0s 5ms/step - loss: 177.0016 - accuracy: 0.2012\n","Epoch 5/50\n","66/66 [==============================] - 0s 5ms/step - loss: 155.6245 - accuracy: 0.1931\n","Epoch 6/50\n","66/66 [==============================] - 0s 5ms/step - loss: 127.4867 - accuracy: 0.2046\n","Epoch 7/50\n","66/66 [==============================] - 0s 4ms/step - loss: 107.7307 - accuracy: 0.2032\n","Epoch 8/50\n","66/66 [==============================] - 0s 4ms/step - loss: 93.5785 - accuracy: 0.2032\n","Epoch 9/50\n","66/66 [==============================] - 0s 5ms/step - loss: 76.2106 - accuracy: 0.2200\n","Epoch 10/50\n","66/66 [==============================] - 0s 5ms/step - loss: 61.3283 - accuracy: 0.2123\n","Epoch 11/50\n","66/66 [==============================] - 0s 4ms/step - loss: 50.4031 - accuracy: 0.2099\n","Epoch 12/50\n","66/66 [==============================] - 0s 4ms/step - loss: 40.3756 - accuracy: 0.1998\n","Epoch 13/50\n","66/66 [==============================] - 0s 4ms/step - loss: 32.8553 - accuracy: 0.1964\n","Epoch 14/50\n","66/66 [==============================] - 0s 3ms/step - loss: 26.4752 - accuracy: 0.2104\n","Epoch 15/50\n","66/66 [==============================] - 0s 3ms/step - loss: 22.2828 - accuracy: 0.1912\n","Epoch 16/50\n","66/66 [==============================] - 0s 3ms/step - loss: 17.0707 - accuracy: 0.1964\n","Epoch 17/50\n","66/66 [==============================] - 0s 3ms/step - loss: 12.4277 - accuracy: 0.1969\n","Epoch 18/50\n","66/66 [==============================] - 0s 3ms/step - loss: 9.0075 - accuracy: 0.1993\n","Epoch 19/50\n","66/66 [==============================] - 0s 3ms/step - loss: 5.7445 - accuracy: 0.2056\n","Epoch 20/50\n","66/66 [==============================] - 0s 3ms/step - loss: 3.0769 - accuracy: 0.2147\n","Epoch 21/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.7163 - accuracy: 0.2104\n","Epoch 22/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6358 - accuracy: 0.1984\n","Epoch 23/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6370 - accuracy: 0.2185\n","Epoch 24/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6385 - accuracy: 0.1912\n","Epoch 25/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6280 - accuracy: 0.1964\n","Epoch 26/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6203 - accuracy: 0.2037\n","Epoch 27/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6251 - accuracy: 0.2003\n","Epoch 28/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6269 - accuracy: 0.2003\n","Epoch 29/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6163 - accuracy: 0.2075\n","Epoch 30/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6145 - accuracy: 0.2123\n","Epoch 31/50\n","66/66 [==============================] - 0s 7ms/step - loss: 1.6215 - accuracy: 0.2109\n","Epoch 32/50\n","66/66 [==============================] - 0s 7ms/step - loss: 1.6202 - accuracy: 0.2113\n","Epoch 33/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6205 - accuracy: 0.2253\n","Epoch 34/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6160 - accuracy: 0.2219\n","Epoch 35/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6123 - accuracy: 0.2185\n","Epoch 36/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6192 - accuracy: 0.2229\n","Epoch 37/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6160 - accuracy: 0.2334\n","Epoch 38/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6147 - accuracy: 0.2329\n","Epoch 39/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6100 - accuracy: 0.2435\n","Epoch 40/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6177 - accuracy: 0.2320\n","Epoch 41/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6159 - accuracy: 0.2349\n","Epoch 42/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6198 - accuracy: 0.2392\n","Epoch 43/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6150 - accuracy: 0.2397\n","Epoch 44/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6144 - accuracy: 0.2354\n","Epoch 45/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6163 - accuracy: 0.2358\n","Epoch 46/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6189 - accuracy: 0.2315\n","Epoch 47/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6109 - accuracy: 0.2440\n","Epoch 48/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6124 - accuracy: 0.2392\n","Epoch 49/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6109 - accuracy: 0.2445\n","Epoch 50/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6100 - accuracy: 0.2416\n","17/17 [==============================] - 0s 2ms/step\n","Accuracy, Precision, and Recall for 40 features\n","Accuracy: 0.2399232245681382\n","Precision: 0.8751804970871554\n","Recall: 0.2399232245681382\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["66/66 [==============================] - 1s 3ms/step - loss: 389.6298 - accuracy: 0.1940\n","Epoch 2/50\n","66/66 [==============================] - 0s 3ms/step - loss: 319.3092 - accuracy: 0.1912\n","Epoch 3/50\n","66/66 [==============================] - 0s 3ms/step - loss: 258.0523 - accuracy: 0.2142\n","Epoch 4/50\n","66/66 [==============================] - 0s 3ms/step - loss: 221.4890 - accuracy: 0.1993\n","Epoch 5/50\n","66/66 [==============================] - 0s 3ms/step - loss: 180.8188 - accuracy: 0.2133\n","Epoch 6/50\n","66/66 [==============================] - 0s 3ms/step - loss: 158.4350 - accuracy: 0.2137\n","Epoch 7/50\n","66/66 [==============================] - 0s 3ms/step - loss: 128.4976 - accuracy: 0.1878\n","Epoch 8/50\n","66/66 [==============================] - 0s 3ms/step - loss: 102.9247 - accuracy: 0.2041\n","Epoch 9/50\n","66/66 [==============================] - 0s 3ms/step - loss: 84.3313 - accuracy: 0.2022\n","Epoch 10/50\n","66/66 [==============================] - 0s 3ms/step - loss: 67.4318 - accuracy: 0.1969\n","Epoch 11/50\n","66/66 [==============================] - 0s 3ms/step - loss: 52.4467 - accuracy: 0.1844\n","Epoch 12/50\n","66/66 [==============================] - 0s 3ms/step - loss: 41.4852 - accuracy: 0.1868\n","Epoch 13/50\n","66/66 [==============================] - 0s 3ms/step - loss: 28.3880 - accuracy: 0.2046\n","Epoch 14/50\n","66/66 [==============================] - 0s 3ms/step - loss: 20.2249 - accuracy: 0.1993\n","Epoch 15/50\n","66/66 [==============================] - 0s 3ms/step - loss: 13.1513 - accuracy: 0.1710\n","Epoch 16/50\n","66/66 [==============================] - 0s 3ms/step - loss: 6.4656 - accuracy: 0.1883\n","Epoch 17/50\n","66/66 [==============================] - 0s 3ms/step - loss: 2.0948 - accuracy: 0.2113\n","Epoch 18/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6137 - accuracy: 0.2171\n","Epoch 19/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6083 - accuracy: 0.2315\n","Epoch 20/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6118 - accuracy: 0.2205\n","Epoch 21/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6125 - accuracy: 0.2233\n","Epoch 22/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6103 - accuracy: 0.2402\n","Epoch 23/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6134 - accuracy: 0.2421\n","Epoch 24/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6079 - accuracy: 0.2296\n","Epoch 25/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6096 - accuracy: 0.2344\n","Epoch 26/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6080 - accuracy: 0.2349\n","Epoch 27/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6069 - accuracy: 0.2430\n","Epoch 28/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6122 - accuracy: 0.2262\n","Epoch 29/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6100 - accuracy: 0.2349\n","Epoch 30/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6075 - accuracy: 0.2402\n","Epoch 31/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6098 - accuracy: 0.2421\n","Epoch 32/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6088 - accuracy: 0.2334\n","Epoch 33/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6082 - accuracy: 0.2291\n","Epoch 34/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6103 - accuracy: 0.2378\n","Epoch 35/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6080 - accuracy: 0.2358\n","Epoch 36/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6103 - accuracy: 0.2378\n","Epoch 37/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6091 - accuracy: 0.2349\n","Epoch 38/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6109 - accuracy: 0.2349\n","Epoch 39/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6081 - accuracy: 0.2329\n","Epoch 40/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6030 - accuracy: 0.2430\n","Epoch 41/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6085 - accuracy: 0.2358\n","Epoch 42/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6084 - accuracy: 0.2421\n","Epoch 43/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6060 - accuracy: 0.2334\n","Epoch 44/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6085 - accuracy: 0.2392\n","Epoch 45/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6100 - accuracy: 0.2267\n","Epoch 46/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6098 - accuracy: 0.2354\n","Epoch 47/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6064 - accuracy: 0.2402\n","Epoch 48/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6073 - accuracy: 0.2344\n","Epoch 49/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6099 - accuracy: 0.2392\n","Epoch 50/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6061 - accuracy: 0.2426\n","17/17 [==============================] - 0s 2ms/step\n","Accuracy, Precision, and Recall for 40 features\n","Accuracy: 0.2399232245681382\n","Precision: 0.9844127298619281\n","Recall: 0.2399232245681382\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","66/66 [==============================] - 1s 3ms/step - loss: 467.7744 - accuracy: 0.1786\n","Epoch 2/50\n","66/66 [==============================] - 0s 3ms/step - loss: 333.6568 - accuracy: 0.1887\n","Epoch 3/50\n","66/66 [==============================] - 0s 3ms/step - loss: 251.8676 - accuracy: 0.1863\n","Epoch 4/50\n","66/66 [==============================] - 0s 3ms/step - loss: 192.2607 - accuracy: 0.1954\n","Epoch 5/50\n","66/66 [==============================] - 0s 3ms/step - loss: 156.3287 - accuracy: 0.2055\n","Epoch 6/50\n","66/66 [==============================] - 0s 3ms/step - loss: 141.0116 - accuracy: 0.1949\n","Epoch 7/50\n","66/66 [==============================] - 0s 3ms/step - loss: 120.3494 - accuracy: 0.1795\n","Epoch 8/50\n","66/66 [==============================] - 0s 3ms/step - loss: 108.1991 - accuracy: 0.1762\n","Epoch 9/50\n","66/66 [==============================] - 0s 4ms/step - loss: 85.0584 - accuracy: 0.2012\n","Epoch 10/50\n","66/66 [==============================] - 0s 3ms/step - loss: 76.8673 - accuracy: 0.1896\n","Epoch 11/50\n","66/66 [==============================] - 0s 3ms/step - loss: 63.5138 - accuracy: 0.2108\n","Epoch 12/50\n","66/66 [==============================] - 0s 3ms/step - loss: 53.8734 - accuracy: 0.2031\n","Epoch 13/50\n","66/66 [==============================] - 0s 4ms/step - loss: 45.0681 - accuracy: 0.2026\n","Epoch 14/50\n","66/66 [==============================] - 0s 3ms/step - loss: 38.7555 - accuracy: 0.1940\n","Epoch 15/50\n","66/66 [==============================] - 0s 3ms/step - loss: 30.0470 - accuracy: 0.1983\n","Epoch 16/50\n","66/66 [==============================] - 0s 3ms/step - loss: 22.8560 - accuracy: 0.1983\n","Epoch 17/50\n","66/66 [==============================] - 0s 3ms/step - loss: 17.8093 - accuracy: 0.1925\n","Epoch 18/50\n","66/66 [==============================] - 0s 3ms/step - loss: 14.2487 - accuracy: 0.1954\n","Epoch 19/50\n","66/66 [==============================] - 0s 3ms/step - loss: 11.1769 - accuracy: 0.1988\n","Epoch 20/50\n","66/66 [==============================] - 0s 3ms/step - loss: 7.8960 - accuracy: 0.1896\n","Epoch 21/50\n","66/66 [==============================] - 0s 3ms/step - loss: 5.1471 - accuracy: 0.1925\n","Epoch 22/50\n","66/66 [==============================] - 0s 3ms/step - loss: 2.5993 - accuracy: 0.1940\n","Epoch 23/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.7089 - accuracy: 0.1964\n","Epoch 24/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6744 - accuracy: 0.1872\n","Epoch 25/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6507 - accuracy: 0.2021\n","Epoch 26/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6493 - accuracy: 0.2175\n","Epoch 27/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6362 - accuracy: 0.2213\n","Epoch 28/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6369 - accuracy: 0.2242\n","Epoch 29/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6406 - accuracy: 0.2160\n","Epoch 30/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6359 - accuracy: 0.2261\n","Epoch 31/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6354 - accuracy: 0.2328\n","Epoch 32/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6281 - accuracy: 0.2352\n","Epoch 33/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6355 - accuracy: 0.2372\n","Epoch 34/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6323 - accuracy: 0.2256\n","Epoch 35/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6261 - accuracy: 0.2328\n","Epoch 36/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6188 - accuracy: 0.2357\n","Epoch 37/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6290 - accuracy: 0.2280\n","Epoch 38/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6276 - accuracy: 0.2266\n","Epoch 39/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6288 - accuracy: 0.2276\n","Epoch 40/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6318 - accuracy: 0.2319\n","Epoch 41/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6277 - accuracy: 0.2400\n","Epoch 42/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6304 - accuracy: 0.2256\n","Epoch 43/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6214 - accuracy: 0.2333\n","Epoch 44/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6202 - accuracy: 0.2400\n","Epoch 45/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6213 - accuracy: 0.2376\n","Epoch 46/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6241 - accuracy: 0.2271\n","Epoch 47/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6125 - accuracy: 0.2429\n","Epoch 48/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6211 - accuracy: 0.2357\n","Epoch 49/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6185 - accuracy: 0.2448\n","Epoch 50/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6164 - accuracy: 0.2372\n","17/17 [==============================] - 0s 3ms/step\n","Accuracy, Precision, and Recall for 40 features\n","Accuracy: 0.19038461538461537\n","Precision: 0.9980769230769231\n","Recall: 0.19038461538461537\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","66/66 [==============================] - 2s 5ms/step - loss: 305.3840 - accuracy: 0.2170\n","Epoch 2/50\n","66/66 [==============================] - 0s 5ms/step - loss: 279.4778 - accuracy: 0.2204\n","Epoch 3/50\n","66/66 [==============================] - 0s 5ms/step - loss: 233.0320 - accuracy: 0.2136\n","Epoch 4/50\n","66/66 [==============================] - 0s 5ms/step - loss: 213.3077 - accuracy: 0.2165\n","Epoch 5/50\n","66/66 [==============================] - 0s 5ms/step - loss: 183.3271 - accuracy: 0.2204\n","Epoch 6/50\n","66/66 [==============================] - 0s 5ms/step - loss: 163.1654 - accuracy: 0.2103\n","Epoch 7/50\n","66/66 [==============================] - 0s 4ms/step - loss: 147.4830 - accuracy: 0.2098\n","Epoch 8/50\n","66/66 [==============================] - 0s 5ms/step - loss: 128.7060 - accuracy: 0.1973\n","Epoch 9/50\n","66/66 [==============================] - 0s 5ms/step - loss: 112.8761 - accuracy: 0.2031\n","Epoch 10/50\n","66/66 [==============================] - 0s 5ms/step - loss: 94.3340 - accuracy: 0.2136\n","Epoch 11/50\n","66/66 [==============================] - 0s 4ms/step - loss: 88.4456 - accuracy: 0.2084\n","Epoch 12/50\n","66/66 [==============================] - 0s 3ms/step - loss: 76.4147 - accuracy: 0.2223\n","Epoch 13/50\n","66/66 [==============================] - 0s 3ms/step - loss: 68.7680 - accuracy: 0.2112\n","Epoch 14/50\n","66/66 [==============================] - 0s 3ms/step - loss: 61.0484 - accuracy: 0.2204\n","Epoch 15/50\n","66/66 [==============================] - 0s 3ms/step - loss: 54.6802 - accuracy: 0.2012\n","Epoch 16/50\n","66/66 [==============================] - 0s 3ms/step - loss: 48.0931 - accuracy: 0.1997\n","Epoch 17/50\n","66/66 [==============================] - 0s 3ms/step - loss: 42.5673 - accuracy: 0.2237\n","Epoch 18/50\n","66/66 [==============================] - 0s 3ms/step - loss: 38.4407 - accuracy: 0.2074\n","Epoch 19/50\n","66/66 [==============================] - 0s 3ms/step - loss: 35.7332 - accuracy: 0.1911\n","Epoch 20/50\n","66/66 [==============================] - 0s 3ms/step - loss: 29.0938 - accuracy: 0.2103\n","Epoch 21/50\n","66/66 [==============================] - 0s 3ms/step - loss: 26.9388 - accuracy: 0.2050\n","Epoch 22/50\n","66/66 [==============================] - 0s 3ms/step - loss: 23.8232 - accuracy: 0.2093\n","Epoch 23/50\n","66/66 [==============================] - 0s 3ms/step - loss: 20.2162 - accuracy: 0.2132\n","Epoch 24/50\n","66/66 [==============================] - 0s 3ms/step - loss: 18.8124 - accuracy: 0.2074\n","Epoch 25/50\n","66/66 [==============================] - 0s 3ms/step - loss: 16.6854 - accuracy: 0.2012\n","Epoch 26/50\n","66/66 [==============================] - 0s 3ms/step - loss: 14.3567 - accuracy: 0.1973\n","Epoch 27/50\n","66/66 [==============================] - 0s 3ms/step - loss: 11.9432 - accuracy: 0.2079\n","Epoch 28/50\n","66/66 [==============================] - 0s 3ms/step - loss: 10.3494 - accuracy: 0.1959\n","Epoch 29/50\n","66/66 [==============================] - 0s 3ms/step - loss: 9.0862 - accuracy: 0.1988\n","Epoch 30/50\n","66/66 [==============================] - 0s 3ms/step - loss: 7.1107 - accuracy: 0.2040\n","Epoch 31/50\n","66/66 [==============================] - 0s 3ms/step - loss: 5.3816 - accuracy: 0.2180\n","Epoch 32/50\n","66/66 [==============================] - 0s 3ms/step - loss: 4.1338 - accuracy: 0.2084\n","Epoch 33/50\n","66/66 [==============================] - 0s 3ms/step - loss: 2.9910 - accuracy: 0.2093\n","Epoch 34/50\n","66/66 [==============================] - 0s 3ms/step - loss: 2.1698 - accuracy: 0.2069\n","Epoch 35/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6843 - accuracy: 0.2271\n","Epoch 36/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6154 - accuracy: 0.2343\n","Epoch 37/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6249 - accuracy: 0.2093\n","Epoch 38/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6158 - accuracy: 0.2242\n","Epoch 39/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6090 - accuracy: 0.2271\n","Epoch 40/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6179 - accuracy: 0.2064\n","Epoch 41/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6175 - accuracy: 0.1978\n","Epoch 42/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6197 - accuracy: 0.2055\n","Epoch 43/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6103 - accuracy: 0.2146\n","Epoch 44/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6076 - accuracy: 0.2021\n","Epoch 45/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6147 - accuracy: 0.2064\n","Epoch 46/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6204 - accuracy: 0.1723\n","Epoch 47/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6061 - accuracy: 0.1983\n","Epoch 48/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6021 - accuracy: 0.2338\n","Epoch 49/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6114 - accuracy: 0.1954\n","Epoch 50/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6141 - accuracy: 0.1964\n","17/17 [==============================] - 0s 2ms/step\n","Accuracy, Precision, and Recall for 40 features\n","Accuracy: 0.2519230769230769\n","Precision: 0.6331229454306379\n","Recall: 0.2519230769230769\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["66/66 [==============================] - 1s 3ms/step - loss: 546.7404 - accuracy: 0.2075\n","Epoch 2/50\n","66/66 [==============================] - 0s 3ms/step - loss: 455.3171 - accuracy: 0.2003\n","Epoch 3/50\n","66/66 [==============================] - 0s 3ms/step - loss: 370.4981 - accuracy: 0.2012\n","Epoch 4/50\n","66/66 [==============================] - 0s 3ms/step - loss: 314.2789 - accuracy: 0.2003\n","Epoch 5/50\n","66/66 [==============================] - 0s 3ms/step - loss: 245.9957 - accuracy: 0.1950\n","Epoch 6/50\n","66/66 [==============================] - 0s 3ms/step - loss: 202.9942 - accuracy: 0.2190\n","Epoch 7/50\n","66/66 [==============================] - 0s 3ms/step - loss: 160.5723 - accuracy: 0.2003\n","Epoch 8/50\n","66/66 [==============================] - 0s 3ms/step - loss: 123.6027 - accuracy: 0.2037\n","Epoch 9/50\n","66/66 [==============================] - 0s 3ms/step - loss: 96.9381 - accuracy: 0.2142\n","Epoch 10/50\n","66/66 [==============================] - 0s 3ms/step - loss: 73.3964 - accuracy: 0.2051\n","Epoch 11/50\n","66/66 [==============================] - 0s 3ms/step - loss: 52.7673 - accuracy: 0.2161\n","Epoch 12/50\n","66/66 [==============================] - 0s 3ms/step - loss: 33.6389 - accuracy: 0.2233\n","Epoch 13/50\n","66/66 [==============================] - 0s 3ms/step - loss: 20.9342 - accuracy: 0.2041\n","Epoch 14/50\n","66/66 [==============================] - 0s 3ms/step - loss: 8.4125 - accuracy: 0.1993\n","Epoch 15/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.7880 - accuracy: 0.2123\n","Epoch 16/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6118 - accuracy: 0.2286\n","Epoch 17/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6108 - accuracy: 0.2363\n","Epoch 18/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6110 - accuracy: 0.2339\n","Epoch 19/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6086 - accuracy: 0.2382\n","Epoch 20/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6106 - accuracy: 0.2411\n","Epoch 21/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6087 - accuracy: 0.2349\n","Epoch 22/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6110 - accuracy: 0.2363\n","Epoch 23/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6097 - accuracy: 0.2402\n","Epoch 24/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6097 - accuracy: 0.2406\n","Epoch 25/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6095 - accuracy: 0.2358\n","Epoch 26/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6102 - accuracy: 0.2373\n","Epoch 27/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6096 - accuracy: 0.2387\n","Epoch 28/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6102 - accuracy: 0.2411\n","Epoch 29/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6098 - accuracy: 0.2363\n","Epoch 30/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6092 - accuracy: 0.2392\n","Epoch 31/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6086 - accuracy: 0.2411\n","Epoch 32/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6080 - accuracy: 0.2430\n","Epoch 33/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6092 - accuracy: 0.2406\n","Epoch 34/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6082 - accuracy: 0.2464\n","Epoch 35/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6074 - accuracy: 0.2445\n","Epoch 36/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6094 - accuracy: 0.2416\n","Epoch 37/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6099 - accuracy: 0.2354\n","Epoch 38/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6091 - accuracy: 0.2402\n","Epoch 39/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6088 - accuracy: 0.2397\n","Epoch 40/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6092 - accuracy: 0.2411\n","Epoch 41/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6087 - accuracy: 0.2382\n","Epoch 42/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6086 - accuracy: 0.2402\n","Epoch 43/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6088 - accuracy: 0.2382\n","Epoch 44/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6081 - accuracy: 0.2286\n","Epoch 45/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6086 - accuracy: 0.2358\n","Epoch 46/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6083 - accuracy: 0.2421\n","Epoch 47/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6090 - accuracy: 0.2397\n","Epoch 48/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6083 - accuracy: 0.2402\n","Epoch 49/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6084 - accuracy: 0.2397\n","Epoch 50/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6085 - accuracy: 0.2397\n","17/17 [==============================] - 0s 2ms/step\n","Accuracy, Precision, and Recall for 40 features\n","Accuracy: 0.2034548944337812\n","Precision: 0.3089165892211212\n","Recall: 0.2034548944337812\n","Epoch 1/50\n","66/66 [==============================] - 1s 3ms/step - loss: 299.7310 - accuracy: 0.2046\n","Epoch 2/50\n","66/66 [==============================] - 0s 3ms/step - loss: 242.3491 - accuracy: 0.2089\n","Epoch 3/50\n","66/66 [==============================] - 0s 3ms/step - loss: 198.7796 - accuracy: 0.1964\n","Epoch 4/50\n","66/66 [==============================] - 0s 3ms/step - loss: 174.1249 - accuracy: 0.2012\n","Epoch 5/50\n","66/66 [==============================] - 0s 3ms/step - loss: 141.4150 - accuracy: 0.1864\n","Epoch 6/50\n","66/66 [==============================] - 0s 3ms/step - loss: 124.0270 - accuracy: 0.1988\n","Epoch 7/50\n","66/66 [==============================] - 0s 3ms/step - loss: 110.5821 - accuracy: 0.1945\n","Epoch 8/50\n","66/66 [==============================] - 0s 3ms/step - loss: 97.6389 - accuracy: 0.1950\n","Epoch 9/50\n","66/66 [==============================] - 0s 3ms/step - loss: 78.7592 - accuracy: 0.2075\n","Epoch 10/50\n","66/66 [==============================] - 0s 3ms/step - loss: 65.6497 - accuracy: 0.1936\n","Epoch 11/50\n","66/66 [==============================] - 0s 3ms/step - loss: 51.9432 - accuracy: 0.2147\n","Epoch 12/50\n","66/66 [==============================] - 0s 3ms/step - loss: 44.7847 - accuracy: 0.2118\n","Epoch 13/50\n","66/66 [==============================] - 0s 3ms/step - loss: 36.4041 - accuracy: 0.2133\n","Epoch 14/50\n","66/66 [==============================] - 0s 3ms/step - loss: 31.1477 - accuracy: 0.1926\n","Epoch 15/50\n","66/66 [==============================] - 0s 3ms/step - loss: 24.9015 - accuracy: 0.2133\n","Epoch 16/50\n","66/66 [==============================] - 0s 3ms/step - loss: 19.9770 - accuracy: 0.1897\n","Epoch 17/50\n","66/66 [==============================] - 0s 3ms/step - loss: 14.6730 - accuracy: 0.1945\n","Epoch 18/50\n","66/66 [==============================] - 0s 3ms/step - loss: 11.5345 - accuracy: 0.2080\n","Epoch 19/50\n","66/66 [==============================] - 0s 3ms/step - loss: 8.4429 - accuracy: 0.2233\n","Epoch 20/50\n","66/66 [==============================] - 0s 3ms/step - loss: 6.1357 - accuracy: 0.2017\n","Epoch 21/50\n","66/66 [==============================] - 0s 3ms/step - loss: 3.7692 - accuracy: 0.2027\n","Epoch 22/50\n","66/66 [==============================] - 0s 3ms/step - loss: 2.3312 - accuracy: 0.1883\n","Epoch 23/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6843 - accuracy: 0.1940\n","Epoch 24/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6454 - accuracy: 0.1945\n","Epoch 25/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6342 - accuracy: 0.1844\n","Epoch 26/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6287 - accuracy: 0.1993\n","Epoch 27/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6330 - accuracy: 0.1936\n","Epoch 28/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6235 - accuracy: 0.1964\n","Epoch 29/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6216 - accuracy: 0.1840\n","Epoch 30/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6237 - accuracy: 0.1691\n","Epoch 31/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6290 - accuracy: 0.2037\n","Epoch 32/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6219 - accuracy: 0.1950\n","Epoch 33/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6198 - accuracy: 0.1940\n","Epoch 34/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6184 - accuracy: 0.2157\n","Epoch 35/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6266 - accuracy: 0.2008\n","Epoch 36/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6199 - accuracy: 0.1916\n","Epoch 37/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6184 - accuracy: 0.2017\n","Epoch 38/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6181 - accuracy: 0.2157\n","Epoch 39/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6159 - accuracy: 0.1984\n","Epoch 40/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6150 - accuracy: 0.2012\n","Epoch 41/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6199 - accuracy: 0.1820\n","Epoch 42/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6105 - accuracy: 0.2008\n","Epoch 43/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6172 - accuracy: 0.1883\n","Epoch 44/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6173 - accuracy: 0.2142\n","Epoch 45/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6133 - accuracy: 0.1950\n","Epoch 46/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6143 - accuracy: 0.2181\n","Epoch 47/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6185 - accuracy: 0.2219\n","Epoch 48/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6174 - accuracy: 0.2104\n","Epoch 49/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6143 - accuracy: 0.2128\n","Epoch 50/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6158 - accuracy: 0.2089\n","17/17 [==============================] - 0s 3ms/step\n","Accuracy, Precision, and Recall for 40 features\n","Accuracy: 0.2571976967370441\n","Precision: 0.9098840437175806\n","Recall: 0.2571976967370441\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","66/66 [==============================] - 2s 5ms/step - loss: 380.1374 - accuracy: 0.2056\n","Epoch 2/50\n","66/66 [==============================] - 0s 5ms/step - loss: 318.3294 - accuracy: 0.2017\n","Epoch 3/50\n","66/66 [==============================] - 0s 5ms/step - loss: 266.5396 - accuracy: 0.2003\n","Epoch 4/50\n","66/66 [==============================] - 0s 5ms/step - loss: 218.1154 - accuracy: 0.1844\n","Epoch 5/50\n","66/66 [==============================] - 0s 5ms/step - loss: 173.6468 - accuracy: 0.2041\n","Epoch 6/50\n","66/66 [==============================] - 0s 5ms/step - loss: 142.6206 - accuracy: 0.2056\n","Epoch 7/50\n","66/66 [==============================] - 0s 5ms/step - loss: 111.1556 - accuracy: 0.2017\n","Epoch 8/50\n","66/66 [==============================] - 0s 3ms/step - loss: 89.5744 - accuracy: 0.1945\n","Epoch 9/50\n","66/66 [==============================] - 0s 3ms/step - loss: 72.9870 - accuracy: 0.2181\n","Epoch 10/50\n","66/66 [==============================] - 0s 3ms/step - loss: 56.3213 - accuracy: 0.2089\n","Epoch 11/50\n","66/66 [==============================] - 0s 3ms/step - loss: 42.1307 - accuracy: 0.2118\n","Epoch 12/50\n","66/66 [==============================] - 0s 3ms/step - loss: 29.4489 - accuracy: 0.1945\n","Epoch 13/50\n","66/66 [==============================] - 0s 3ms/step - loss: 18.9878 - accuracy: 0.1998\n","Epoch 14/50\n","66/66 [==============================] - 0s 3ms/step - loss: 10.7446 - accuracy: 0.2185\n","Epoch 15/50\n","66/66 [==============================] - 0s 3ms/step - loss: 5.5884 - accuracy: 0.1931\n","Epoch 16/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.9782 - accuracy: 0.2137\n","Epoch 17/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6328 - accuracy: 0.2185\n","Epoch 18/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6280 - accuracy: 0.2205\n","Epoch 19/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6281 - accuracy: 0.2267\n","Epoch 20/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6215 - accuracy: 0.2248\n","Epoch 21/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6232 - accuracy: 0.2253\n","Epoch 22/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6196 - accuracy: 0.2209\n","Epoch 23/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6312 - accuracy: 0.2349\n","Epoch 24/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6181 - accuracy: 0.2320\n","Epoch 25/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6161 - accuracy: 0.2378\n","Epoch 26/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6206 - accuracy: 0.2253\n","Epoch 27/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6197 - accuracy: 0.2272\n","Epoch 28/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6208 - accuracy: 0.2334\n","Epoch 29/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6189 - accuracy: 0.2334\n","Epoch 30/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6176 - accuracy: 0.2281\n","Epoch 31/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6226 - accuracy: 0.2296\n","Epoch 32/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6216 - accuracy: 0.2257\n","Epoch 33/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6160 - accuracy: 0.2214\n","Epoch 34/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6133 - accuracy: 0.2378\n","Epoch 35/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6117 - accuracy: 0.2296\n","Epoch 36/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6197 - accuracy: 0.2277\n","Epoch 37/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6148 - accuracy: 0.2315\n","Epoch 38/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6158 - accuracy: 0.2315\n","Epoch 39/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6136 - accuracy: 0.2267\n","Epoch 40/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6178 - accuracy: 0.2224\n","Epoch 41/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6188 - accuracy: 0.2291\n","Epoch 42/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6151 - accuracy: 0.2329\n","Epoch 43/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6082 - accuracy: 0.2325\n","Epoch 44/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6114 - accuracy: 0.2286\n","Epoch 45/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6177 - accuracy: 0.2219\n","Epoch 46/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6134 - accuracy: 0.2286\n","Epoch 47/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6112 - accuracy: 0.2368\n","Epoch 48/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6066 - accuracy: 0.2344\n","Epoch 49/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6144 - accuracy: 0.2272\n","Epoch 50/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6100 - accuracy: 0.2329\n","17/17 [==============================] - 0s 2ms/step\n","Accuracy, Precision, and Recall for 40 features\n","Accuracy: 0.236084452975048\n","Precision: 0.9881276701133058\n","Recall: 0.236084452975048\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["66/66 [==============================] - 2s 5ms/step - loss: 434.8769 - accuracy: 0.1992\n","Epoch 2/50\n","66/66 [==============================] - 0s 5ms/step - loss: 355.9875 - accuracy: 0.1978\n","Epoch 3/50\n","66/66 [==============================] - 0s 5ms/step - loss: 315.1132 - accuracy: 0.1940\n","Epoch 4/50\n","66/66 [==============================] - 0s 6ms/step - loss: 265.9748 - accuracy: 0.1829\n","Epoch 5/50\n","66/66 [==============================] - 0s 5ms/step - loss: 230.0423 - accuracy: 0.2031\n","Epoch 6/50\n","66/66 [==============================] - 0s 5ms/step - loss: 208.8234 - accuracy: 0.1959\n","Epoch 7/50\n","66/66 [==============================] - 0s 5ms/step - loss: 176.4281 - accuracy: 0.1930\n","Epoch 8/50\n","66/66 [==============================] - 0s 4ms/step - loss: 144.8455 - accuracy: 0.2098\n","Epoch 9/50\n","66/66 [==============================] - 0s 7ms/step - loss: 124.4440 - accuracy: 0.1882\n","Epoch 10/50\n","66/66 [==============================] - 1s 17ms/step - loss: 100.0718 - accuracy: 0.2002\n","Epoch 11/50\n","66/66 [==============================] - 0s 7ms/step - loss: 88.9230 - accuracy: 0.2136\n","Epoch 12/50\n","66/66 [==============================] - 0s 5ms/step - loss: 77.9294 - accuracy: 0.1954\n","Epoch 13/50\n","66/66 [==============================] - 0s 5ms/step - loss: 70.3149 - accuracy: 0.1925\n","Epoch 14/50\n","66/66 [==============================] - 0s 5ms/step - loss: 56.9897 - accuracy: 0.2012\n","Epoch 15/50\n","66/66 [==============================] - 0s 5ms/step - loss: 49.8093 - accuracy: 0.1930\n","Epoch 16/50\n","66/66 [==============================] - 0s 6ms/step - loss: 43.1915 - accuracy: 0.2175\n","Epoch 17/50\n","66/66 [==============================] - 0s 5ms/step - loss: 35.0801 - accuracy: 0.2069\n","Epoch 18/50\n","66/66 [==============================] - 0s 4ms/step - loss: 28.9331 - accuracy: 0.2007\n","Epoch 19/50\n","66/66 [==============================] - 0s 3ms/step - loss: 21.8855 - accuracy: 0.2103\n","Epoch 20/50\n","66/66 [==============================] - 0s 3ms/step - loss: 16.0995 - accuracy: 0.1916\n","Epoch 21/50\n","66/66 [==============================] - 0s 3ms/step - loss: 10.1376 - accuracy: 0.2031\n","Epoch 22/50\n","66/66 [==============================] - 0s 3ms/step - loss: 5.3312 - accuracy: 0.2036\n","Epoch 23/50\n","66/66 [==============================] - 0s 3ms/step - loss: 2.3554 - accuracy: 0.2098\n","Epoch 24/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6559 - accuracy: 0.1983\n","Epoch 25/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6242 - accuracy: 0.2040\n","Epoch 26/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6325 - accuracy: 0.2093\n","Epoch 27/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6292 - accuracy: 0.2088\n","Epoch 28/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6276 - accuracy: 0.2064\n","Epoch 29/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6274 - accuracy: 0.2060\n","Epoch 30/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6229 - accuracy: 0.2055\n","Epoch 31/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6145 - accuracy: 0.2088\n","Epoch 32/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6222 - accuracy: 0.2141\n","Epoch 33/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6111 - accuracy: 0.2088\n","Epoch 34/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6300 - accuracy: 0.2055\n","Epoch 35/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6168 - accuracy: 0.2208\n","Epoch 36/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6220 - accuracy: 0.2122\n","Epoch 37/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6102 - accuracy: 0.2036\n","Epoch 38/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6190 - accuracy: 0.2002\n","Epoch 39/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6124 - accuracy: 0.2031\n","Epoch 40/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6112 - accuracy: 0.2088\n","Epoch 41/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6123 - accuracy: 0.2141\n","Epoch 42/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6120 - accuracy: 0.2151\n","Epoch 43/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6147 - accuracy: 0.2391\n","Epoch 44/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6135 - accuracy: 0.2194\n","Epoch 45/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6180 - accuracy: 0.2175\n","Epoch 46/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6103 - accuracy: 0.2266\n","Epoch 47/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6160 - accuracy: 0.2194\n","Epoch 48/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6138 - accuracy: 0.2348\n","Epoch 49/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6098 - accuracy: 0.2453\n","Epoch 50/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6053 - accuracy: 0.2434\n","17/17 [==============================] - 0s 2ms/step\n","Accuracy, Precision, and Recall for 40 features\n","Accuracy: 0.17884615384615385\n","Precision: 0.8913455338987255\n","Recall: 0.17884615384615385\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["66/66 [==============================] - 1s 3ms/step - loss: 230.2980 - accuracy: 0.2007\n","Epoch 2/50\n","66/66 [==============================] - 0s 3ms/step - loss: 194.3119 - accuracy: 0.2112\n","Epoch 3/50\n","66/66 [==============================] - 0s 3ms/step - loss: 155.2877 - accuracy: 0.2199\n","Epoch 4/50\n","66/66 [==============================] - 0s 3ms/step - loss: 121.8150 - accuracy: 0.2146\n","Epoch 5/50\n","66/66 [==============================] - 0s 3ms/step - loss: 107.1599 - accuracy: 0.2021\n","Epoch 6/50\n","66/66 [==============================] - 0s 3ms/step - loss: 86.2931 - accuracy: 0.2103\n","Epoch 7/50\n","66/66 [==============================] - 0s 3ms/step - loss: 70.2816 - accuracy: 0.2127\n","Epoch 8/50\n","66/66 [==============================] - 0s 3ms/step - loss: 58.3434 - accuracy: 0.2012\n","Epoch 9/50\n","66/66 [==============================] - 0s 3ms/step - loss: 48.6954 - accuracy: 0.2016\n","Epoch 10/50\n","66/66 [==============================] - 0s 4ms/step - loss: 39.8320 - accuracy: 0.2026\n","Epoch 11/50\n","66/66 [==============================] - 0s 4ms/step - loss: 33.7969 - accuracy: 0.1892\n","Epoch 12/50\n","66/66 [==============================] - 0s 5ms/step - loss: 26.9688 - accuracy: 0.2079\n","Epoch 13/50\n","66/66 [==============================] - 0s 5ms/step - loss: 22.9336 - accuracy: 0.2127\n","Epoch 14/50\n","66/66 [==============================] - 0s 5ms/step - loss: 19.8391 - accuracy: 0.2112\n","Epoch 15/50\n","66/66 [==============================] - 0s 4ms/step - loss: 16.0768 - accuracy: 0.1819\n","Epoch 16/50\n","66/66 [==============================] - 0s 4ms/step - loss: 13.0469 - accuracy: 0.1964\n","Epoch 17/50\n","66/66 [==============================] - 0s 5ms/step - loss: 9.2311 - accuracy: 0.1959\n","Epoch 18/50\n","66/66 [==============================] - 0s 5ms/step - loss: 6.2553 - accuracy: 0.1997\n","Epoch 19/50\n","66/66 [==============================] - 0s 5ms/step - loss: 3.4438 - accuracy: 0.2093\n","Epoch 20/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.9522 - accuracy: 0.1863\n","Epoch 21/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6594 - accuracy: 0.1930\n","Epoch 22/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6307 - accuracy: 0.2141\n","Epoch 23/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6294 - accuracy: 0.2127\n","Epoch 24/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6206 - accuracy: 0.2218\n","Epoch 25/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6367 - accuracy: 0.1997\n","Epoch 26/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6241 - accuracy: 0.2146\n","Epoch 27/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6240 - accuracy: 0.2141\n","Epoch 28/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6203 - accuracy: 0.2223\n","Epoch 29/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6250 - accuracy: 0.2213\n","Epoch 30/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6173 - accuracy: 0.2237\n","Epoch 31/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6188 - accuracy: 0.2295\n","Epoch 32/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6191 - accuracy: 0.2333\n","Epoch 33/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6233 - accuracy: 0.2232\n","Epoch 34/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6193 - accuracy: 0.2261\n","Epoch 35/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6129 - accuracy: 0.2338\n","Epoch 36/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6182 - accuracy: 0.2304\n","Epoch 37/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6201 - accuracy: 0.2280\n","Epoch 38/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6179 - accuracy: 0.2333\n","Epoch 39/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6169 - accuracy: 0.2372\n","Epoch 40/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6140 - accuracy: 0.2381\n","Epoch 41/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6127 - accuracy: 0.2252\n","Epoch 42/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6189 - accuracy: 0.2309\n","Epoch 43/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6140 - accuracy: 0.2309\n","Epoch 44/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6154 - accuracy: 0.2300\n","Epoch 45/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6124 - accuracy: 0.2362\n","Epoch 46/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6149 - accuracy: 0.2252\n","Epoch 47/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6112 - accuracy: 0.2319\n","Epoch 48/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6177 - accuracy: 0.2228\n","Epoch 49/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6137 - accuracy: 0.2295\n","Epoch 50/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6143 - accuracy: 0.2276\n","17/17 [==============================] - 0s 2ms/step\n","Accuracy, Precision, and Recall for 40 features\n","Accuracy: 0.2423076923076923\n","Precision: 0.9980769230769231\n","Recall: 0.2423076923076923\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","66/66 [==============================] - 1s 3ms/step - loss: 643.4594 - accuracy: 0.2094\n","Epoch 2/50\n","66/66 [==============================] - 0s 4ms/step - loss: 536.2585 - accuracy: 0.1960\n","Epoch 3/50\n","66/66 [==============================] - 0s 3ms/step - loss: 432.6747 - accuracy: 0.2051\n","Epoch 4/50\n","66/66 [==============================] - 0s 4ms/step - loss: 360.1970 - accuracy: 0.1955\n","Epoch 5/50\n","66/66 [==============================] - 0s 4ms/step - loss: 305.8428 - accuracy: 0.2046\n","Epoch 6/50\n","66/66 [==============================] - 0s 3ms/step - loss: 259.8650 - accuracy: 0.1782\n","Epoch 7/50\n","66/66 [==============================] - 0s 3ms/step - loss: 211.6404 - accuracy: 0.2022\n","Epoch 8/50\n","66/66 [==============================] - 0s 3ms/step - loss: 183.1549 - accuracy: 0.1950\n","Epoch 9/50\n","66/66 [==============================] - 0s 4ms/step - loss: 145.4745 - accuracy: 0.2267\n","Epoch 10/50\n","66/66 [==============================] - 0s 4ms/step - loss: 136.6084 - accuracy: 0.2099\n","Epoch 11/50\n","66/66 [==============================] - 0s 4ms/step - loss: 114.5037 - accuracy: 0.2012\n","Epoch 12/50\n","66/66 [==============================] - 0s 3ms/step - loss: 100.0108 - accuracy: 0.2109\n","Epoch 13/50\n","66/66 [==============================] - 0s 3ms/step - loss: 89.8201 - accuracy: 0.2003\n","Epoch 14/50\n","66/66 [==============================] - 0s 3ms/step - loss: 78.5319 - accuracy: 0.2075\n","Epoch 15/50\n","66/66 [==============================] - 0s 3ms/step - loss: 71.2873 - accuracy: 0.2008\n","Epoch 16/50\n","66/66 [==============================] - 0s 3ms/step - loss: 61.3048 - accuracy: 0.2118\n","Epoch 17/50\n","66/66 [==============================] - 0s 3ms/step - loss: 54.8041 - accuracy: 0.2109\n","Epoch 18/50\n","66/66 [==============================] - 0s 3ms/step - loss: 50.9010 - accuracy: 0.2094\n","Epoch 19/50\n","66/66 [==============================] - 0s 5ms/step - loss: 44.7410 - accuracy: 0.2051\n","Epoch 20/50\n","66/66 [==============================] - 0s 5ms/step - loss: 38.4779 - accuracy: 0.1960\n","Epoch 21/50\n","66/66 [==============================] - 0s 5ms/step - loss: 32.7377 - accuracy: 0.2147\n","Epoch 22/50\n","66/66 [==============================] - 0s 5ms/step - loss: 28.0384 - accuracy: 0.2181\n","Epoch 23/50\n","66/66 [==============================] - 0s 5ms/step - loss: 23.5439 - accuracy: 0.2109\n","Epoch 24/50\n","66/66 [==============================] - 0s 5ms/step - loss: 21.3108 - accuracy: 0.1921\n","Epoch 25/50\n","66/66 [==============================] - 0s 5ms/step - loss: 17.5230 - accuracy: 0.2003\n","Epoch 26/50\n","66/66 [==============================] - 0s 7ms/step - loss: 13.2637 - accuracy: 0.2070\n","Epoch 27/50\n","66/66 [==============================] - 1s 15ms/step - loss: 9.7025 - accuracy: 0.2003\n","Epoch 28/50\n","66/66 [==============================] - 0s 7ms/step - loss: 6.6415 - accuracy: 0.2152\n","Epoch 29/50\n","66/66 [==============================] - 0s 5ms/step - loss: 3.7528 - accuracy: 0.2003\n","Epoch 30/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.9215 - accuracy: 0.2061\n","Epoch 31/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6277 - accuracy: 0.2070\n","Epoch 32/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6118 - accuracy: 0.2099\n","Epoch 33/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6093 - accuracy: 0.2190\n","Epoch 34/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6085 - accuracy: 0.2085\n","Epoch 35/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6081 - accuracy: 0.1993\n","Epoch 36/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6096 - accuracy: 0.2094\n","Epoch 37/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6085 - accuracy: 0.2037\n","Epoch 38/50\n","66/66 [==============================] - 0s 5ms/step - loss: 1.6086 - accuracy: 0.1969\n","Epoch 39/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6103 - accuracy: 0.1960\n","Epoch 40/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6087 - accuracy: 0.2022\n","Epoch 41/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6093 - accuracy: 0.1988\n","Epoch 42/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6099 - accuracy: 0.1921\n","Epoch 43/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6078 - accuracy: 0.2012\n","Epoch 44/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6071 - accuracy: 0.2094\n","Epoch 45/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6097 - accuracy: 0.1974\n","Epoch 46/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6083 - accuracy: 0.1979\n","Epoch 47/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6082 - accuracy: 0.2037\n","Epoch 48/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6087 - accuracy: 0.1955\n","Epoch 49/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6067 - accuracy: 0.2113\n","Epoch 50/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.6073 - accuracy: 0.2032\n","17/17 [==============================] - 0s 3ms/step\n","Accuracy, Precision, and Recall for 40 features\n","Accuracy: 0.21113243761996162\n","Precision: 0.6384106428616902\n","Recall: 0.21113243761996162\n","Epoch 1/50\n","66/66 [==============================] - 1s 4ms/step - loss: 574.8260 - accuracy: 0.1907\n","Epoch 2/50\n","66/66 [==============================] - 0s 5ms/step - loss: 475.2082 - accuracy: 0.1945\n","Epoch 3/50\n","66/66 [==============================] - 0s 5ms/step - loss: 407.9536 - accuracy: 0.1873\n","Epoch 4/50\n","66/66 [==============================] - 0s 5ms/step - loss: 345.0242 - accuracy: 0.2032\n","Epoch 5/50\n","66/66 [==============================] - 0s 5ms/step - loss: 309.6086 - accuracy: 0.1868\n","Epoch 6/50\n","66/66 [==============================] - 0s 5ms/step - loss: 259.3542 - accuracy: 0.1849\n","Epoch 7/50\n","66/66 [==============================] - 0s 5ms/step - loss: 224.4383 - accuracy: 0.1988\n","Epoch 8/50\n","66/66 [==============================] - 0s 5ms/step - loss: 194.0475 - accuracy: 0.1825\n","Epoch 9/50\n","66/66 [==============================] - 0s 5ms/step - loss: 169.8457 - accuracy: 0.1936\n","Epoch 10/50\n","66/66 [==============================] - 0s 5ms/step - loss: 144.2958 - accuracy: 0.2123\n","Epoch 11/50\n","66/66 [==============================] - 0s 5ms/step - loss: 122.4135 - accuracy: 0.2046\n","Epoch 12/50\n","66/66 [==============================] - 0s 5ms/step - loss: 102.4141 - accuracy: 0.2190\n","Epoch 13/50\n","66/66 [==============================] - 0s 5ms/step - loss: 90.1296 - accuracy: 0.2046\n","Epoch 14/50\n","66/66 [==============================] - 0s 5ms/step - loss: 80.3309 - accuracy: 0.1955\n","Epoch 15/50\n","66/66 [==============================] - 0s 5ms/step - loss: 67.4899 - accuracy: 0.2099\n","Epoch 16/50\n","66/66 [==============================] - 0s 5ms/step - loss: 57.3872 - accuracy: 0.2195\n","Epoch 17/50\n","66/66 [==============================] - 0s 5ms/step - loss: 50.7006 - accuracy: 0.2056\n","Epoch 18/50\n","66/66 [==============================] - 0s 5ms/step - loss: 39.6198 - accuracy: 0.2061\n","Epoch 19/50\n","66/66 [==============================] - 0s 6ms/step - loss: 32.5802 - accuracy: 0.1998\n","Epoch 20/50\n","66/66 [==============================] - 0s 4ms/step - loss: 26.3604 - accuracy: 0.1840\n","Epoch 21/50\n","66/66 [==============================] - 0s 4ms/step - loss: 17.7144 - accuracy: 0.2123\n","Epoch 22/50\n","66/66 [==============================] - 0s 5ms/step - loss: 11.0675 - accuracy: 0.2185\n","Epoch 23/50\n","66/66 [==============================] - 0s 5ms/step - loss: 5.4012 - accuracy: 0.2046\n","Epoch 24/50\n","66/66 [==============================] - 0s 4ms/step - loss: 1.7738 - accuracy: 0.1998\n","Epoch 25/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6344 - accuracy: 0.2104\n","Epoch 26/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6265 - accuracy: 0.2085\n","Epoch 27/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6302 - accuracy: 0.2200\n","Epoch 28/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6230 - accuracy: 0.2133\n","Epoch 29/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6264 - accuracy: 0.2157\n","Epoch 30/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6167 - accuracy: 0.2224\n","Epoch 31/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6260 - accuracy: 0.2104\n","Epoch 32/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6186 - accuracy: 0.2248\n","Epoch 33/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6227 - accuracy: 0.2267\n","Epoch 34/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6187 - accuracy: 0.2195\n","Epoch 35/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6239 - accuracy: 0.2161\n","Epoch 36/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6169 - accuracy: 0.2205\n","Epoch 37/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6259 - accuracy: 0.2147\n","Epoch 38/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6202 - accuracy: 0.2123\n","Epoch 39/50\n","66/66 [==============================] - 0s 3ms/step - loss: 1.6206 - accuracy: 0.2205\n","Epoch 40/50\n"," 1/66 [..............................] - ETA: 0s - loss: 1.6245 - accuracy: 0.3125"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-b05a09755627>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlabel_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_assess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtraining_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1781\u001b[0m                         ):\n\u001b[1;32m   1782\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1783\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1784\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    868\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1263\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mflat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;34m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1480\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[0;32m---> 60\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"AzrcwAEQdD9b","executionInfo":{"status":"aborted","timestamp":1701183283860,"user_tz":-420,"elapsed":13,"user":{"displayName":"Nurul Andini M277BSX1256","userId":"07500154941621038947"}}},"source":["history_df = pd.DataFrame(training_history)\n","history_df.to_csv('history.csv')"],"execution_count":null,"outputs":[]}]}